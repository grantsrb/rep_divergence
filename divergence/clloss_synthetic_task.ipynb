{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8435a29-eaa6-4d36-b3f8-c5541cc16f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import torch\n",
    "from toytask_utils import make_tasks, all_divisions\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fcb0f-3b32-4201-baf4-84bc2b057456",
   "metadata": {},
   "source": [
    "# Generate Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58296a45-029e-4fb7-849a-3812de8ef8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100 # number of times to resample each class\n",
    "x_values = [-1,1] # valid causal x values\n",
    "y_values = [i for i in range(5)] # valid causal y values\n",
    "std = 0.1 # standard deviation of noise along causal dims\n",
    "cov = 0.2 # covariance between causal dims\n",
    "\n",
    "varbs = []\n",
    "for x in x_values:\n",
    "    for y in y_values:\n",
    "        varbs.append([x,y])\n",
    "varbs = np.asarray(varbs)\n",
    "\n",
    "samples = []\n",
    "og_varbs = []\n",
    "xmean = 0\n",
    "ymean = 0\n",
    "for _ in range(n_samples):\n",
    "    samp = varbs.copy().astype(float)\n",
    "    og_varbs.append(varbs.copy())\n",
    "    samp[:,0] += cov*samp[:,1]\n",
    "    noise = std*np.random.randn(*samp.shape)\n",
    "    samp = samp + noise\n",
    "    samples.append(samp)\n",
    "samples = np.vstack(samples)\n",
    "samples = samples - samples.mean(0)\n",
    "og_varbs = np.vstack(og_varbs)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe6d2d-8587-4698-bb07-5f5a72436a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"x\": samples[:,0],\n",
    "    \"y\": samples[:,1],\n",
    "    \"hue\": samples[:,1],\n",
    "})\n",
    "df[\"x\"] = (df[\"x\"]-np.mean(df[\"x\"]))\n",
    "df[\"hue\"] = df[\"hue\"]-np.min(df[\"hue\"])\n",
    "df[\"hue\"] = df[\"hue\"]/np.max(df[\"hue\"])\n",
    "\n",
    "\n",
    "fontsize=25\n",
    "legendsize = 25\n",
    "alpha = 0.8\n",
    "dark = 0.2\n",
    "light = 0.85\n",
    "rot = 0\n",
    "thickness = 2\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "intrv_df = df.copy()\n",
    "intrv_df[\"y\"] = np.asarray(intrv_df[\"y\"])[np.random.permutation(len(intrv_df)).astype(int)]\n",
    "intrv_cmap = sns.cubehelix_palette(start=-.3, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "#intrv_cmap = sns.dark_palette(\"blue\", as_cmap=True)\n",
    "sns.scatterplot(x=\"x\", y=\"y\", alpha=alpha, data=intrv_df, ax=ax, hue=\"hue\", palette=intrv_cmap, edgecolor=\"none\")\n",
    "\n",
    "native_cmap = sns.cubehelix_palette(start=0.7, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "#native_cmap = sns.dark_palette(\"red\", as_cmap=True)\n",
    "sns.scatterplot(x=\"x\", y=\"y\", alpha=alpha, data=df, ax=ax, hue=\"hue\", palette=native_cmap, edgecolor=\"none\")\n",
    "                #hue=\"hue\", palette=\"blue\")\n",
    "    \n",
    "## y divider\n",
    "ax.plot([0,0],[-3,3], \"k--\", alpha=0.5, linewidth=thickness)\n",
    "# x dividers\n",
    "for i in y_values[:-1]:\n",
    "    y = i+0.5-2\n",
    "    ax.plot([-2,2],[y,y], \"k--\", alpha=0.5, linewidth=thickness)\n",
    "plt.xlim([-2,2])\n",
    "plt.ylim([-2.75,2.75])\n",
    "\n",
    "plt.xlabel(\"\", fontsize=fontsize)\n",
    "plt.ylabel(\"\", fontsize=fontsize)\n",
    "\n",
    "plt.xticks([], fontsize=fontsize)\n",
    "plt.yticks([], fontsize=fontsize)\n",
    "\n",
    "# # Manually create colorbars / legend patches\n",
    "# native_cmap = sns.cubehelix_palette(start=0.7, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "# intrv_cmap = sns.cubehelix_palette(start=-.3, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "\n",
    "# Legend handles: colored rectangles with labels\n",
    "#native_patch = mpatches.Patch(color=native_cmap(0.8), label=\"Native\")\n",
    "#intrv_patch = mpatches.Patch(color=intrv_cmap(0.8), label=\"Intervened\")\n",
    "\n",
    "#ax.legend(handles=[native_patch, intrv_patch], fontsize=legendsize, loc=\"upper right\", bbox_to_anchor=(1.75,1))\n",
    "plt.legend().set_visible(False)\n",
    "#plt.savefig(\"figs/example_divergence.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b42ce7-e878-4e1a-a504-bba181af9d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib as mpl\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"x\": samples[:,0],\n",
    "    \"y\": samples[:,1],\n",
    "    \"hue\": samples[:,1],\n",
    "})\n",
    "df[\"x\"] = (df[\"x\"]-np.mean(df[\"x\"]))\n",
    "df[\"hue\"] = df[\"hue\"]-np.min(df[\"hue\"])\n",
    "df[\"hue\"] = df[\"hue\"]/np.max(df[\"hue\"])\n",
    "\n",
    "\n",
    "\n",
    "fontsize=25\n",
    "legendsize = 25\n",
    "alpha = 0.8\n",
    "dark = 0.2\n",
    "light = 0.85\n",
    "rot = 0\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "intrv_df = df.copy()\n",
    "intrv_df[\"y\"] = np.asarray(intrv_df[\"y\"])[np.random.permutation(len(intrv_df)).astype(int)]\n",
    "intrv_cmap = sns.cubehelix_palette(start=-.3, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "#intrv_cmap = sns.dark_palette(\"blue\", as_cmap=True)\n",
    "sns.scatterplot(x=\"x\", y=\"y\", alpha=alpha, data=intrv_df, ax=ax, hue=\"hue\", palette=intrv_cmap, edgecolor=\"none\")\n",
    "\n",
    "native_cmap = sns.cubehelix_palette(start=0.7, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "#native_cmap = sns.dark_palette(\"red\", as_cmap=True)\n",
    "sns.scatterplot(x=\"x\", y=\"y\", alpha=alpha, data=df, ax=ax, hue=\"hue\", palette=native_cmap, edgecolor=\"none\")\n",
    "                #hue=\"hue\", palette=\"blue\")\n",
    "    \n",
    "## y divider\n",
    "#ax.plot([0,0],[-1,5], \"k--\", alpha=0.5)\n",
    "## x dividers\n",
    "#for i in y_values[:-1]:\n",
    "#    y = i+0.5\n",
    "#    ax.plot([-2,2],[y,y], \"k--\", alpha=0.5)\n",
    "plt.xlim([-2,2])\n",
    "plt.ylim([-2.75,2.75])\n",
    "\n",
    "plt.xlabel(\"\", fontsize=fontsize)\n",
    "plt.ylabel(\"\", fontsize=fontsize)\n",
    "\n",
    "plt.xticks([], fontsize=fontsize)\n",
    "plt.yticks([], fontsize=fontsize)\n",
    "\n",
    "# # Manually create colorbars / legend patches\n",
    "# native_cmap = sns.cubehelix_palette(start=0.7, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "# intrv_cmap = sns.cubehelix_palette(start=-.3, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "\n",
    "# Legend handles: colored rectangles with labels\n",
    "native_patch = mpatches.Patch(color=native_cmap(0.8), label=\"Native\")\n",
    "intrv_patch = mpatches.Patch(color=intrv_cmap(0.8), label=\"Intervened\")\n",
    "\n",
    "ax.legend(handles=[native_patch, intrv_patch], fontsize=legendsize, loc=\"upper right\", bbox_to_anchor=(1.75,1))\n",
    "#plt.savefig(\"figs/legend.png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f70f5e-f9f4-4e62-897b-4dce7b2a644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomloss import SamplesLoss\n",
    "kwargs = {\n",
    "    \"loss\": \"sinkhorn\",\n",
    "    \"p\": 2,\n",
    "    \"blur\": 0.05,\n",
    "}\n",
    "loss_fn = SamplesLoss(**kwargs)\n",
    "\n",
    "def compute_emd(X,Y):\n",
    "    return loss_fn(X.float(),Y.float())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822aedf-72af-4e05-9471-6c6443c96b7d",
   "metadata": {},
   "source": [
    "# CL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a803c06-cc9c-4e91-a5f8-196f9ef5234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(og_varbs.shape)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4cfe7-0d9e-44b4-8a6d-2add360dfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_plot(\n",
    "    natty,\n",
    "    intrv,\n",
    "    natty_classes=None,\n",
    "    intrv_classes=None,\n",
    "    save_name=None,\n",
    "    incl_legend=False,\n",
    "    incl_dividers=True,\n",
    "    xlim=[-2,2],\n",
    "    ylim=[-2.75,2.75],\n",
    "    labels=[\"Native\", \"Intervened\"],\n",
    "    intrv_cmap=None,\n",
    "    native_cmap=None,\n",
    "    intrv_color=None,\n",
    "    native_color=None,\n",
    "    intrv_alpha = 0.95,\n",
    "    native_alpha = 0.6,\n",
    "    thickness=2,\n",
    "    dash_alpha=0.5,\n",
    "):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    natty = natty.cpu().detach().numpy()\n",
    "    intrv = intrv.cpu().detach().numpy()\n",
    "    \n",
    "    if natty_classes is None:\n",
    "        if intrv_cmap is None:\n",
    "            intrv_cmap = sns.cubehelix_palette(start=-.3, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "        if intrv_color is None:\n",
    "            intrv_color = intrv_cmap(0.8)\n",
    "        sns.scatterplot(x=intrv[:,0], y=intrv[:,1], alpha=intrv_alpha, ax=ax, color=intrv_color, edgecolor=\"none\")\n",
    "    elif len(set(intrv_classes))==1:\n",
    "        if intrv_cmap is None:\n",
    "            intrv_cmap = sns.cubehelix_palette(start=-.3, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "        if intrv_color is None:\n",
    "            intrv_color = intrv_cmap(0.8)\n",
    "        sns.scatterplot(x=intrv[:,0], y=intrv[:,1], alpha=intrv_alpha, ax=ax, color=intrv_color, edgecolor=\"none\")\n",
    "    else:\n",
    "        if intrv_cmap is None:\n",
    "            intrv_cmap = sns.color_palette(\"pastel\")\n",
    "        sns.scatterplot(\n",
    "            x=intrv[:,0], y=intrv[:,1],\n",
    "            alpha=intrv_alpha, ax=ax,\n",
    "            hue=intrv_classes, palette=intrv_cmap\n",
    "        )\n",
    "\n",
    "    if natty_classes is None:\n",
    "        if native_cmap is None:\n",
    "            native_cmap = sns.cubehelix_palette(start=0.7, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "        if native_color is None:\n",
    "            native_color = native_cmap(0.8)\n",
    "        sns.scatterplot(x=natty[:,0], y=natty[:,1], alpha=native_alpha, ax=ax, color=native_color, edgecolor=\"none\")\n",
    "    elif len(set(natty_classes))==1:\n",
    "        if native_cmap is None:\n",
    "            native_cmap = sns.cubehelix_palette(start=0.7, rot=rot, dark=dark, light=light, reverse=True, as_cmap=True)\n",
    "        if native_color is None:\n",
    "            native_color = native_cmap(0.8)\n",
    "        sns.scatterplot(x=natty[:,0], y=natty[:,1], alpha=native_alpha, ax=ax, color=native_color, edgecolor=\"none\")\n",
    "    else:\n",
    "        if native_cmap is None:\n",
    "            native_cmap = sns.color_palette(\"dark\")\n",
    "        sns.scatterplot(\n",
    "            x=natty[:,0],\n",
    "            y=natty[:,1],\n",
    "            alpha=native_alpha,\n",
    "            ax=ax,\n",
    "            hue=natty_classes,\n",
    "            palette=native_cmap,\n",
    "            edgecolor=\"none\"\n",
    "        )\n",
    "        \n",
    "    if incl_legend and natty_classes is None:\n",
    "        native_patch = mpatches.Patch(color=native_color, label=labels[0])\n",
    "        intrv_patch = mpatches.Patch(color=intrv_color, label=labels[1])\n",
    "        ax.legend(handles=[native_patch, intrv_patch], fontsize=legendsize, loc=\"upper right\", bbox_to_anchor=(1.75,1))\n",
    "    elif incl_legend:\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.legend().set_visible(False)\n",
    "        \n",
    "    ## y divider\n",
    "    if incl_dividers:\n",
    "        ax.plot([0,0],[-3,3], \"k--\", linewidth=thickness, alpha=dash_alpha)\n",
    "        # x dividers\n",
    "        for i in y_values[:-1]:\n",
    "            y = i+0.5-2\n",
    "            ax.plot([-2,2],[y,y], \"k--\", linewidth=thickness, alpha=dash_alpha)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    if save_name:\n",
    "        plt.savefig(save_name, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6a507-bf6e-4f87-988e-fd93b4ad43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot_fwd(vecs, mtx):\n",
    "    return torch.matmul(vecs, mtx.weight )\n",
    "\n",
    "def rot_bck(vecs, mtx):\n",
    "    inv = torch.linalg.inv(mtx.weight)\n",
    "    return torch.matmul(vecs, inv )\n",
    "\n",
    "def interchange(trg,src,mtx,mask):\n",
    "    \"\"\"\n",
    "    Patches the non-zero masked values from the src\n",
    "    into the trg vectors in the rotated space.\n",
    "    \n",
    "    Args:\n",
    "        trg: torch tensor (B,D)\n",
    "        src: torch tensor (B,D)\n",
    "        mtx: torch module (D,D)\n",
    "            needs \"weight\" property\n",
    "        mask: torch tensor (D,)\n",
    "            ones denote dimensions that will be transferred\n",
    "    \"\"\"\n",
    "    rot_trg = rot_fwd(trg, mtx)\n",
    "    rot_src = rot_fwd(src, mtx )\n",
    "    intrv = rot_trg*(1-mask) + rot_src*mask\n",
    "    return rot_bck(intrv, mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a869-45b9-406e-b76c-bcf9e2b02d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fn(vecs, eps=1e-7):\n",
    "    return (vecs-vecs.mean(0))/(vecs.std(0)+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67912541-c970-4d43-a3c2-1f9c501c02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_from_varbs(varbs, v2class=None):\n",
    "    if v2class is None:\n",
    "        v2class = dict()\n",
    "    classes = []\n",
    "    for v in varbs.detach().cpu().tolist():\n",
    "        tup = tuple(v)\n",
    "        if tup not in v2class:\n",
    "            v2class[tup] = len(v2class)\n",
    "        classes.append(v2class[tup])\n",
    "    return np.asarray(classes), v2class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252115ef-3aec-443b-a5a3-f447bad35b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_dims = 64\n",
    "cov = 0\n",
    "mask_dims = 1\n",
    "\n",
    "og_varbs = torch.tensor(og_varbs).long()\n",
    "d = og_varbs.shape[-1] + extra_dims\n",
    "mask = torch.zeros(d)\n",
    "mask[:mask_dims] = 1\n",
    "\n",
    "samples = torch.tensor(samples).float()\n",
    "noise = torch.randn(len(og_varbs), extra_dims) + cov * torch.randn(len(og_varbs), extra_dims)*og_varbs[:,1:]\n",
    "natty_varbs = og_varbs.clone()\n",
    "natty_classes, v2class = get_classes_from_varbs(natty_varbs)\n",
    "natty_samps = torch.cat([ samples, noise ], dim=-1)\n",
    "perm = torch.randperm(len(og_varbs)).long()\n",
    "\n",
    "intrv_varbs = torch.cat([ natty_varbs[:,0:1], natty_varbs[perm,1:] ], dim=-1)\n",
    "intrv_classes, _ = get_classes_from_varbs(intrv_varbs, v2class=v2class)\n",
    "\n",
    "trg_vecs = natty_samps[perm].clone()\n",
    "src_vecs = natty_samps.clone()\n",
    "\n",
    "eye = torch.nn.Linear(d,d)\n",
    "eye.weight.data = torch.eye(d).float()\n",
    "with torch.no_grad():\n",
    "    intrv_samps = interchange(trg_vecs, src_vecs, eye, mask)\n",
    "\n",
    "# Sanity check\n",
    "quick_plot(\n",
    "    natty_samps,\n",
    "    intrv_samps,\n",
    "    natty_classes=natty_classes,\n",
    "    intrv_classes=intrv_classes,\n",
    "    dash_alpha=0.2,\n",
    "    thickness=3,\n",
    "    save_name=\"figs/identity_patching.png\",\n",
    ")\n",
    "\n",
    "rot_mtx = torch.nn.utils.parametrizations.orthogonal(torch.nn.Linear(d,d))\n",
    "\n",
    "with torch.no_grad():\n",
    "    intrv_samps = interchange(trg_vecs, src_vecs, rot_mtx, mask)\n",
    "\n",
    "# Sanity check\n",
    "quick_plot(natty_samps, intrv_samps, natty_classes=natty_classes, intrv_classes=intrv_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d371d0-6bc2-4809-bbb7-48532a90319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(iv,nv) in enumerate(zip(intrv_varbs,natty_varbs)):\n",
    "    if tuple(iv.tolist())==tuple(nv.tolist()):\n",
    "        assert natty_classes[i]==intrv_classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601b4b5-8c1c-4004-bfbd-ed5600c35d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "    vecs, classes,\n",
    "    n_epochs=1000,\n",
    "    lr=0.01,\n",
    "    l2=0.01,\n",
    "    drop_p=0.5,\n",
    "    bsize=128,\n",
    "    patience=500,\n",
    "    print_every=50,\n",
    "    n_layers=3,\n",
    "    hidden_dim=256,\n",
    "    pre_layernorm=False,\n",
    "    pre_batchnorm=True,\n",
    "    layernorm=False,\n",
    "    batchnorm=True,\n",
    "    model=None,\n",
    "    ret_best=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    vecs: torch tensor (B,D)\n",
    "    classes: list-like (B,)\n",
    "    \"\"\"\n",
    "    classes = torch.tensor(classes).long()\n",
    "    d = vecs.shape[-1]\n",
    "    n = len(set(classes.detach().cpu().tolist()))\n",
    "    if model is None:\n",
    "        modules = []\n",
    "        if pre_layernorm:\n",
    "            modules.append(torch.nn.LayerNorm(d))\n",
    "        if pre_batchnorm:\n",
    "            modules.append(torch.nn.BatchNorm1d(d))\n",
    "        if n_layers>2:\n",
    "            modules += [\n",
    "                torch.nn.Linear(d,hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(drop_p),\n",
    "            ]\n",
    "            if layernorm:\n",
    "                modules.append( torch.nn.LayerNorm(hidden_dim) )\n",
    "            if batchnorm:\n",
    "                modules.append( torch.nn.BatchNorm1d(hidden_dim) )\n",
    "            d = hidden_dim\n",
    "        if n_layers>1:\n",
    "            modules += [\n",
    "                torch.nn.Linear(d,hidden_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(drop_p),\n",
    "            ]\n",
    "            if layernorm:\n",
    "                modules.append( torch.nn.LayerNorm(hidden_dim) )\n",
    "            if batchnorm:\n",
    "                modules.append( torch.nn.BatchNorm1d(hidden_dim) )\n",
    "            d = hidden_dim\n",
    "        modules.append(torch.nn.Linear(d,n))\n",
    "        model = torch.nn.Sequential(*modules)\n",
    "    if verbose:\n",
    "        print(model)\n",
    "    model.train()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2)\n",
    "    optim.zero_grad()\n",
    "\n",
    "    d = vecs.shape[-1]\n",
    "    tlen = int(len(vecs)*0.8)\n",
    "    train_vecs = vecs[:tlen]\n",
    "    train_classes = classes[:tlen]\n",
    "    valid_vecs = vecs[tlen:]\n",
    "    valid_classes = classes[tlen:]\n",
    "    prev_loss = np.inf\n",
    "    best_loss = 0\n",
    "    best_valid_loss = 0\n",
    "    best_train_acc = 0\n",
    "    best_valid_acc = 0\n",
    "    n_pat = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        perm = torch.randperm(len(train_vecs)).long()\n",
    "        for b in range(0,len(perm)-bsize+1,bsize):\n",
    "            idxs = perm[b:b+bsize]\n",
    "            inputs = train_vecs[idxs]\n",
    "            labels = train_classes[idxs]\n",
    "            preds = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            preds = model(train_vecs)\n",
    "            train_acc = (preds.argmax(-1)==train_classes).float().mean()\n",
    "            model.eval()\n",
    "            preds = model(valid_vecs)\n",
    "            valid_acc = (preds.argmax(-1)==valid_classes).float().mean()\n",
    "            valid_loss = torch.nn.functional.cross_entropy(preds, valid_classes)\n",
    "        if epoch % print_every == 0 and verbose:\n",
    "            print(epoch,\n",
    "                  \"TrnLoss:\", loss.item(),\n",
    "                  \"ValLoss:\", valid_loss.item(),\n",
    "                  \"TrnAcc:\", train_acc.item(),\n",
    "                  \"ValAcc:\", valid_acc.item()\n",
    "                )\n",
    "        if valid_acc>best_valid_acc or (valid_acc>=best_valid_acc and train_acc>best_train_acc):\n",
    "            best_loss = loss\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            best_train_acc = train_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        if valid_loss>=prev_loss:\n",
    "            n_pat += 1\n",
    "            if n_pat>=patience:\n",
    "                print(\"Converged at epoch\", epoch)\n",
    "                break\n",
    "        prev_loss = valid_loss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    if ret_best:\n",
    "        print(epoch,\n",
    "              \"TrnLoss:\", best_loss.item(),\n",
    "              \"ValLoss:\", best_valid_loss.item(),\n",
    "              \"TrnAcc:\", best_train_acc.item(),\n",
    "              \"ValAcc:\", best_valid_acc.item()\n",
    "            )\n",
    "        return best_model, best_train_acc.item(), best_valid_acc.item()\n",
    "    print(epoch,\n",
    "          \"TrnLoss:\", loss.item(),\n",
    "          \"ValLoss:\", valid_loss.item(),\n",
    "          \"TrnAcc:\", train_acc.item(),\n",
    "          \"ValAcc:\", valid_acc.item()\n",
    "        )\n",
    "    return model, train_acc.item(), valid_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da078f-2bcc-4c84-b9c5-5cefbcb8f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cl_vectors(natty_varbs, intrv_varbs, natty_vecs, method=\"sample\"):\n",
    "    \"\"\"\n",
    "    natty_varbs: tensor (B,2)\n",
    "        the non-noisy variable values\n",
    "    intrv_varbs: tensor (B,2)\n",
    "        the non-noisy variable values\n",
    "    natty_vecs: tensor (B,D)\n",
    "        the noisy native vector representations\n",
    "    method: str\n",
    "        options: sample, mean\n",
    "        determines whether the cl vectors should be averaged\n",
    "        over all possible candidates or individual samples\n",
    "    \"\"\"\n",
    "    cl_vectors = []\n",
    "    all_idxs = torch.arange(len(natty_varbs)).long()\n",
    "    for intrv in intrv_varbs:\n",
    "        valid_bools = (natty_varbs[:,0]==intrv[0])&(natty_varbs[:,1]==intrv[1])\n",
    "        valid_idxs = all_idxs[valid_bools]\n",
    "        if method==\"sample\":\n",
    "            idx = valid_idxs[int(np.random.randint(len(valid_idxs)))]\n",
    "            cl_vectors.append(natty_vecs[idx])\n",
    "        elif method in {\"average\", \"mean\"}:\n",
    "            cl_vectors.append(natty_vecs[valid_idxs].mean(0))\n",
    "    return torch.vstack(cl_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab134d-c649-4d06-bc9a-91a9406e22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cl_loss(intrv, cl, cl_loss_type=\"both\"):\n",
    "    l2,cos = 0,0\n",
    "    if cl_loss_type in {\"mse\", \"both\"}:\n",
    "        l2 = ((intrv-cl)**2).mean()\n",
    "    if cl_loss_type in {\"cos\", \"both\"}:\n",
    "        cos = 1-torch.nn.functional.cosine_similarity(intrv,cl)\n",
    "    return l2 + cos\n",
    "    \n",
    "def get_cl_loss(\n",
    "    trg, src,\n",
    "    mtx, mask,\n",
    "    cl_vecs,\n",
    "    empty_mask=None,\n",
    "    incl_extra=True,\n",
    "    n_varbs=2,\n",
    "    calc_loss_in_aligned_basis=False,\n",
    "    detach_cl_vecs=False,\n",
    "    cl_loss_type=\"both\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs the interchange and computes the cl loss\n",
    "    \n",
    "    Args:\n",
    "        trg: tensor (B,D)\n",
    "            target vectors which will be patched into\n",
    "        src: tensor (B,D)\n",
    "            source vectors from which activity will be harvested\n",
    "        mtx: torch module\n",
    "            must have attribute \"weight\"\n",
    "        cl_vecs: tensor (B,D)\n",
    "        incl_extra: bool\n",
    "            if true, the extraneous dimensions are included in\n",
    "            the CL loss\n",
    "        n_varbs: int\n",
    "            the number of variables in the causal abstraction\n",
    "        calc_loss_in_aligned_basis: bool\n",
    "            if true, will compute cl loss in aligned basis\n",
    "        detach_cl_vecs: bool\n",
    "            if true, will detach the cl vectors from gradient\n",
    "            calculations. otherwise, their rotation will be\n",
    "            included in the loss.\n",
    "        cl_loss_type: str\n",
    "            options:\n",
    "                \"cos\": cosine loss only\n",
    "                \"mse\": mse loss only\n",
    "                \"both\": add both cos and mse losses and divide by 2\n",
    "    \"\"\"\n",
    "    intrv = interchange(trg, src, mtx, mask)\n",
    "    if empty_mask is not None:\n",
    "        perm = torch.randperm(len(src)).long()\n",
    "        intrv = interchange(intrv, src[perm], mtx, empty_mask)\n",
    "        \n",
    "    raw_intrv = intrv.clone()\n",
    "    if not incl_extra:\n",
    "        extra_dim = mask.long().sum()*n_varbs\n",
    "        intrv = rot_fwd(intrv, mtx)\n",
    "        intrv[:,extra_dim:] = 0\n",
    "        cl_vecs = rot_fwd(cl_vecs, mtx)\n",
    "        cl_vecs[:,extra_dim:] = 0\n",
    "        if not calc_loss_in_aligned_basis:\n",
    "            intrv = rot_bck(intrv, mtx)\n",
    "            cl_vecs = rot_bck(cl_vecs, mtx)\n",
    "        if detach_cl_vecs: cl_vecs = cl_vecs.detach().data\n",
    "    return calc_cl_loss(intrv, cl_vecs, cl_loss_type=cl_loss_type).mean(), raw_intrv\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d926f53-8c6b-4281-93b3-d94fee705f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actn_loss(preds, labels):\n",
    "    device = preds.get_device()\n",
    "    if device<0: device = \"cpu\"\n",
    "    labels = torch.tensor(labels).long().to(device)\n",
    "    loss = torch.nn.functional.cross_entropy(preds, labels)\n",
    "    acc = (preds.argmax(-1)==labels).float().mean()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169144c-5f8f-41a7-aa01-7c486281f996",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c78d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(\n",
    "    og_varbs,\n",
    "    samples,\n",
    "    v2class=None,\n",
    "    extra_dims = 128, # total number of additional noise dimensions\n",
    "    dupl_rank = 0, # number of additional dimensions that are exact duplicates\n",
    "    zero_rank = 0, # number of dimensions to zero out\n",
    "    mask_dims=1, # number of dimensions in the DAS mask\n",
    "    cov_strength = 1, # how much do the extraneous dimensions covary with the x and y values,\n",
    "    n_samples_per_class=None,\n",
    "    normalize=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Constructs the dataset from the samples\n",
    "    \"\"\"\n",
    "    dupl_rank = min(extra_dims-1, dupl_rank)\n",
    "    \n",
    "    d = og_varbs.shape[-1] + extra_dims\n",
    "    mask = torch.zeros(d)\n",
    "    mask[:mask_dims] = 1\n",
    "\n",
    "    if extra_dims>0:\n",
    "        n_noise_dims = extra_dims-dupl_rank\n",
    "        if extra_dims>1:\n",
    "            noise = torch.randn(len(og_varbs), n_noise_dims) +\\\n",
    "                torch.cat([\n",
    "                    cov * torch.randn(len(og_varbs), n_noise_dims//2)*og_varbs[:,1:],\n",
    "                    cov * torch.randn(len(og_varbs), n_noise_dims//2)*og_varbs[:,0:1]\n",
    "                ],dim=-1)\n",
    "        else:\n",
    "            noise = torch.randn(len(og_varbs), n_noise_dims) + cov * torch.randn(len(og_varbs), n_noise_dims)*og_varbs[:,1:]\n",
    "            \n",
    "        if dupl_rank>0:\n",
    "            idxs = torch.randint(0,noise.shape[-1],(dupl_rank,)).long()\n",
    "            dupls = noise.T[idxs].T\n",
    "            noise = torch.cat([noise,dupls],dim=-1)\n",
    "        if zero_rank>0:\n",
    "            # Instead of just zeroing out dimensions, we reduce the rank of the noise\n",
    "            # in a rotated basis.\n",
    "            n = noise.shape[-1]\n",
    "            orth = torch.nn.utils.parametrizations.orthogonal(torch.nn.Linear(n,n))\n",
    "            with torch.no_grad():\n",
    "                noise = torch.matmul(noise, orth.weight)\n",
    "                noise[:,:zero_rank] = 0\n",
    "                noise = torch.matmul(noise, orth.weight.T)\n",
    "        natty_vecs = torch.cat([\n",
    "            torch.tensor(samples).float(), noise\n",
    "        ], dim=-1)\n",
    "    else: natty_vecs = torch.tensor(samples).float()\n",
    "    natty_varbs = torch.tensor(og_varbs).long()\n",
    "    natty_classes, v2class = get_classes_from_varbs(natty_varbs, v2class=v2class)\n",
    "    natty_classes = torch.tensor(natty_classes).long()\n",
    "    if normalize:\n",
    "        natty_vecs = (natty_vecs-natty_vecs.mean(0))/natty_vecs.std()\n",
    "\n",
    "    if not n_samples_per_class or n_samples_per_class<0:\n",
    "        perm = torch.randperm(len(og_varbs)).long()\n",
    "        intrv_idxs = torch.stack([\n",
    "            torch.arange(len(og_varbs)).long(), perm\n",
    "        ],dim=1)\n",
    "    \n",
    "        intrv_varbs = torch.cat([ natty_varbs[:,0:1], natty_varbs[perm,1:] ], dim=-1)\n",
    "        intrv_classes, intrv_v2class = get_classes_from_varbs(intrv_varbs, v2class={**v2class})\n",
    "        intrv_classes = torch.tensor(intrv_classes).long()\n",
    "        if len(intrv_v2class)!=len(v2class):\n",
    "            valid_intrvs = torch.isin(intrv_classes, torch.tensor(list(v2class.values())).long())\n",
    "        else:\n",
    "            valid_intrvs = torch.ones(len(intrv_classes)).bool()\n",
    "\n",
    "        trg_vecs = natty_vecs[perm].clone()\n",
    "        src_vecs = natty_vecs.clone()\n",
    "    else:\n",
    "        intrv_v2class = {**v2class}\n",
    "        c2varb = {v:k for k,v in v2class.items()}\n",
    "        n_classes = len(v2class)\n",
    "        intrv_classes = []\n",
    "        intrv_idxs = []\n",
    "        intrv_varbs = []\n",
    "        arange = torch.arange(len(natty_varbs)).long()\n",
    "        for c,varb_tup in c2varb.items():\n",
    "            for samp in range(n_samples_per_class):\n",
    "                idxs1 = arange[(natty_varbs[:,0]==varb_tup[0])]\n",
    "                idx1 = idxs1[int(np.random.randint(len(idxs1)))]\n",
    "                idxs2 = arange[(natty_varbs[:,1]==varb_tup[1])]\n",
    "                idx2 = idxs2[int(np.random.randint(len(idxs2)))]\n",
    "                intrv_idxs.append([int(idx1),int(idx2)])\n",
    "                intrv_classes.append(c)\n",
    "                intrv_varbs.append([varb_tup[0], varb_tup[1]])\n",
    "                assert natty_varbs[idx1,0]==varb_tup[0] and natty_varbs[idx2,1]==varb_tup[1]\n",
    "        intrv_idxs = torch.tensor(intrv_idxs).long()\n",
    "        intrv_classes = torch.tensor(intrv_classes).long()\n",
    "        intrv_varbs = torch.tensor(intrv_varbs).long()\n",
    "        valid_intrvs = torch.ones(len(intrv_classes)).bool()\n",
    "        \n",
    "        trg_vecs = natty_vecs[intrv_idxs[:,1]].clone()\n",
    "        src_vecs = natty_vecs[intrv_idxs[:,0]].clone()\n",
    "        natty_varbs = natty_varbs[intrv_idxs[:,0]]\n",
    "        natty_classes = natty_classes[intrv_idxs[:,0]]\n",
    "        natty_vecs = src_vecs.clone()\n",
    "                             \n",
    "    return {\n",
    "        \"intrv_idxs\": intrv_idxs,\n",
    "        \n",
    "        \"mask\": mask.clone(),\n",
    "        \"src_vecs\": src_vecs.clone(),\n",
    "        \"trg_vecs\": trg_vecs.clone(),\n",
    "        \"intrv_varbs\": intrv_varbs.clone(),\n",
    "        \"intrv_classes\": intrv_classes.clone(),\n",
    "        \"valid_intrvs\": valid_intrvs.clone(),\n",
    "        \"src_varbs\": natty_varbs.clone(),\n",
    "        \"intrv_v2class\": intrv_v2class,\n",
    "        \"src_classes\": natty_classes.clone(),\n",
    "        \"v2class\": v2class,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62aea28-8064-41b9-ac26-c88fdff54488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_save_name(params, excl_keys={\"calc_loss_in_aligned_basis\", \"lr\", \"n_epochs\", \"detach_cl_vecs\"}):\n",
    "    s = \"toydiv\"\n",
    "    for k in sorted(list(params.keys())):\n",
    "        if k in excl_keys: continue\n",
    "        v = params[k]\n",
    "        s+= f\"_{k}{v}\"\n",
    "    return s+\".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af5827-9384-4b95-8b31-f916653d564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_rotation(\n",
    "    src_varbs, intrv_varbs, classifier,\n",
    "    trg_vecs, src_vecs,\n",
    "    src_classes, intrv_classes,\n",
    "    mask=None,\n",
    "    n_epochs = 1000,\n",
    "    lr = 0.01,\n",
    "    cl_eps = 1,\n",
    "    cl_loss_type=\"both\",#\"cos\" \"mse\" \"both\"\n",
    "    method = \"sample\",  #\"mean\" \"sample\"\n",
    "    incl_extra = False, # Will include the extraneous subspaces in cl loss if True,\n",
    "    calc_loss_in_aligned_basis = False,\n",
    "    detach_cl_vecs = False,\n",
    "    incl_actn_loss = False,\n",
    "    incl_cl_loss = True,\n",
    "    mtx_type = \"orthog\", # \"orthog\" \"linear\",\n",
    "    print_every = 50,\n",
    "    fig_every = 200,\n",
    "    save_fig=False,\n",
    "    shuffle_empty=False,\n",
    "    incl_dividers=False,\n",
    "    early_stopping=True,\n",
    "    early_stop_thresh=1e-5,\n",
    "    early_stop_patience=100,\n",
    "    **kwargs,\n",
    "):\n",
    "    exp_params = {\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"method\": method,\n",
    "        \"incl_extra\": incl_extra,\n",
    "        \"calc_loss_in_aligned_basis\": calc_loss_in_aligned_basis,\n",
    "        \"detach_cl_vecs\": detach_cl_vecs,\n",
    "        \"incl_actn_loss\": incl_actn_loss,\n",
    "        \"incl_cl_loss\": incl_cl_loss,\n",
    "        \"mtx_type\": mtx_type,\n",
    "        \"cl_eps\": cl_eps,\n",
    "    }\n",
    "\n",
    "    d = src_vecs.shape[-1]\n",
    "    assert incl_actn_loss or incl_cl_loss\n",
    "    cl_vecs = get_cl_vectors(src_varbs, intrv_varbs, src_vecs, method=method)\n",
    "    if mask is None:\n",
    "        mask = torch.zeros(d)\n",
    "        mask[:1] = 1\n",
    "    empty_mask = None\n",
    "    if shuffle_empty:\n",
    "        empty_mask = torch.zeros_like(mask).cuda()\n",
    "        empty_mask[int(mask.long().sum())*2:] = 1\n",
    "\n",
    "    # eye = torch.nn.Linear(d,d)\n",
    "    # eye.weight.data = torch.eye(d).float()\n",
    "    # rot_mtx = eye\n",
    "    if mtx_type==\"linear\":\n",
    "        rot_mtx = SymmetricDefiniteMatrix( size=d, )\n",
    "    elif \"orthog\" in mtx_type:\n",
    "        rot_mtx = torch.nn.utils.parametrizations.orthogonal(torch.nn.Linear(d,d))\n",
    "    optim = torch.optim.Adam(rot_mtx.parameters(), lr=lr)\n",
    "    optim.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eye = torch.nn.Linear(d,d)\n",
    "        eye.weight.data = torch.eye(d).float()\n",
    "        intrv_vecs = interchange(trg_vecs, src_vecs, eye, mask)\n",
    "    print(\"Identity\")\n",
    "    #quick_plot(intrv_vecs, src_vecs, intrv_classes, src_classes, )\n",
    "    if fig_every<np.inf:\n",
    "        quick_plot(\n",
    "            src_vecs, intrv_vecs.detach(),\n",
    "            src_classes, intrv_classes,\n",
    "            save_name=\"figs/identity.png\",\n",
    "            incl_dividers=incl_dividers\n",
    "        )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        intrv_vecs = interchange(trg_vecs, src_vecs, rot_mtx, mask)\n",
    "    print(\"Untrained\")\n",
    "    #quick_plot(intrv_vecs, src_vecs, intrv_classes, src_classes, )\n",
    "    if fig_every<np.inf:\n",
    "        quick_plot(src_vecs, intrv_vecs.detach(), src_classes, intrv_classes, incl_dividers=incl_dividers)\n",
    "\n",
    "    print(\"Training Rotation\")\n",
    "    n_pat = 0\n",
    "    best_acc = 0\n",
    "    best_loss = np.inf\n",
    "    best_emd = np.inf\n",
    "    best_row_emd = np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        cl_loss, intrv_vecs = get_cl_loss(\n",
    "            trg=trg_vecs.cuda(),\n",
    "            src=src_vecs.cuda(),\n",
    "            mtx=rot_mtx.cuda(),\n",
    "            mask=mask.cuda(),\n",
    "            empty_mask=empty_mask,\n",
    "            cl_vecs=cl_vecs.cuda(),\n",
    "            incl_extra=incl_extra,\n",
    "            calc_loss_in_aligned_basis=calc_loss_in_aligned_basis,\n",
    "            detach_cl_vecs=detach_cl_vecs,\n",
    "            cl_loss_type=cl_loss_type,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            perm = torch.randperm(len(src_vecs)).long()\n",
    "            emd = compute_emd(src_vecs[perm].cuda(), intrv_vecs.cuda()).item()\n",
    "            extra_mask = torch.zeros_like(mask).cuda()\n",
    "            extra_mask[:2] = 1 # Only using the causal dimensions\n",
    "            row_emd = compute_emd(\n",
    "                intrv_vecs.cuda()*extra_mask, src_vecs.cuda()*extra_mask\n",
    "            ).item()\n",
    "            \n",
    "        actn_loss, acc = get_actn_loss(classifier(intrv_vecs), intrv_classes)\n",
    "        loss = 0\n",
    "        if incl_actn_loss:\n",
    "            loss += actn_loss\n",
    "        if incl_cl_loss:\n",
    "            loss += cl_eps*cl_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print(epoch, \"Cl Loss:\", cl_loss.item(), \"ActLoss:\", actn_loss.item(), \"Actn:\", acc.item(), \"EMD:\", emd, \"RowEMD\", row_emd)\n",
    "        if epoch % fig_every == 0 and epoch > 0:\n",
    "            quick_plot(src_vecs, intrv_vecs.detach(), src_classes, intrv_classes, incl_dividers=incl_dividers)\n",
    "            \n",
    "        if acc>best_acc:\n",
    "            best_acc = acc\n",
    "            best_mtx = copy.deepcopy(rot_mtx)\n",
    "            best_cl_loss = cl_loss\n",
    "            best_actn_loss = actn_loss\n",
    "            best_intrv_vecs = intrv_vecs.detach().cpu().data.clone()\n",
    "            best_emd = emd\n",
    "            best_row_emd = row_emd\n",
    "        if loss<best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_mtx = copy.deepcopy(rot_mtx)\n",
    "            best_cl_loss = cl_loss\n",
    "            best_actn_loss = actn_loss\n",
    "            best_intrv_vecs = intrv_vecs.detach().cpu().data.clone()\n",
    "            best_emd = emd\n",
    "            best_row_emd = row_emd\n",
    "        if loss>=(best_loss-early_stop_thresh) and early_stopping:\n",
    "            n_pat += 1\n",
    "            if n_pat>early_stop_patience:\n",
    "                print(\"Converged at epoch\", epoch)\n",
    "                break\n",
    "        else:\n",
    "            n_pat = 0\n",
    "        if acc.item()==1 and early_stopping:\n",
    "            print(\"Converged at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "\n",
    "    for p in sorted(list(exp_params.keys())):\n",
    "        print(p, exp_params[p])\n",
    "    print()\n",
    "    if fig_every<np.inf or save_fig:\n",
    "        save_name = \"figs/\"+get_plot_save_name(exp_params)\n",
    "        print(\"Best Plot by Train Loss\")\n",
    "        quick_plot(\n",
    "            src_vecs, best_intrv_vecs,\n",
    "            src_classes, intrv_classes,\n",
    "            save_name=save_name if save_fig else None,\n",
    "            incl_dividers=incl_dividers,\n",
    "        )\n",
    "        print(\"Last Plot\")\n",
    "        quick_plot(\n",
    "            src_vecs, intrv_vecs,\n",
    "            src_classes, intrv_classes,\n",
    "            incl_dividers=incl_dividers,\n",
    "        )\n",
    "    print(epoch,\n",
    "        \"Cl Loss:\", best_cl_loss.item(),\n",
    "          \"ActLoss:\", best_actn_loss.item(),\n",
    "          \"Actn:\", best_acc.item(),\n",
    "          \"EMD:\", best_emd,\n",
    "         \"RowEMD:\", best_row_emd)\n",
    "    return best_mtx, exp_params, best_cl_loss, best_actn_loss, best_acc, best_emd, best_row_emd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9e805-74a7-41bc-8119-39a01d2f3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dims = 64\n",
    "dupl_rank = 0 # duplicates uniformly sampled extra dimensions from the set of extra_dims-dupl_rank noisy dimensions\n",
    "zero_rank = 0\n",
    "mask_dims = 1\n",
    "cov_strength = 0 # how much do the extraneous dimensions covary with the x and y values\n",
    "normalize = False\n",
    "\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "data_dict = prep_data(\n",
    "    og_varbs=og_varbs,\n",
    "    samples=samples,\n",
    "    v2class=v2class,\n",
    "    extra_dims=extra_dims,\n",
    "    dupl_rank=dupl_rank,\n",
    "    zero_rank=zero_rank,\n",
    "    mask_dims=mask_dims,\n",
    "    cov_strength=cov_strength,\n",
    "    n_samples_per_class=100,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "mask = data_dict[\"mask\"]\n",
    "valids = data_dict[\"valid_intrvs\"]\n",
    "src_vecs = data_dict[\"src_vecs\"][valids]\n",
    "trg_vecs = data_dict[\"trg_vecs\"][valids]\n",
    "intrv_idxs = data_dict[\"intrv_idxs\"][valids]\n",
    "intrv_varbs = data_dict[\"intrv_varbs\"][valids]\n",
    "intrv_classes = data_dict[\"intrv_classes\"][valids]\n",
    "src_classes = data_dict[\"src_classes\"][valids]\n",
    "src_varbs = data_dict[\"src_varbs\"][valids]\n",
    "\n",
    "\n",
    "d = src_vecs.shape[-1]\n",
    "mask = torch.zeros(d)\n",
    "mask[:mask_dims] = 1\n",
    "empty_mask = torch.zeros_like(mask)\n",
    "if len(mask)>mask_dims*2:\n",
    "    empty_mask[mask_dims*2:] = 1\n",
    "\n",
    "rot_mtx = torch.nn.utils.parametrizations.orthogonal(torch.nn.Linear(d,d))\n",
    "\n",
    "with torch.no_grad():\n",
    "    eye = torch.nn.Linear(d,d)\n",
    "    eye.weight.data = torch.eye(d).float()\n",
    "    intrv_vecs = interchange(trg_vecs, src_vecs, eye, mask)\n",
    "print(\"Identity\")\n",
    "div = compute_emd(src_vecs, intrv_vecs)\n",
    "print(\"Div:\", div)\n",
    "quick_plot(src_vecs, intrv_vecs.detach(), src_classes, intrv_classes, )\n",
    "\n",
    "with torch.no_grad():\n",
    "    intrv_vecs = interchange(trg_vecs, src_vecs, rot_mtx, mask)\n",
    "print(\"Untrained\")\n",
    "div = compute_emd(src_vecs, intrv_vecs)\n",
    "print(\"Div:\", div)\n",
    "quick_plot(src_vecs, intrv_vecs.detach(), src_classes, intrv_classes, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57632bb4-d604-4fda-84a3-aca382bf2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "bsize = 200\n",
    "patience = 400\n",
    "l2 = 0.005\n",
    "n_epochs = 1000\n",
    "drop_p = 0.5\n",
    "hidden_dim = 128\n",
    "n_layers = 2 # 1-3 layers \n",
    "pre_layernorm = False\n",
    "pre_batchnorm = True\n",
    "layernorm = False\n",
    "batchnorm = True\n",
    "\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "print(\"Training Classifier\")\n",
    "classifier, max_acc, _ = train_classifier(\n",
    "    #normalize_fn(src_vecs),\n",
    "    src_vecs,\n",
    "    src_classes,\n",
    "    lr=lr,\n",
    "    patience=patience,\n",
    "    l2=l2,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_epochs=n_epochs,\n",
    "    drop_p=drop_p,\n",
    "    bsize=bsize,\n",
    "    n_layers=n_layers,\n",
    "    pre_layernorm=pre_layernorm,\n",
    "    pre_batchnorm=pre_batchnorm,\n",
    "    layernorm=layernorm,\n",
    "    batchnorm=batchnorm,\n",
    "    #model=model,\n",
    "    ret_best=True,\n",
    "    verbose=True,\n",
    ")\n",
    "classifier.cuda()\n",
    "classifier.eval()\n",
    "for p in classifier.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ef917-f4bf-4474-9f5e-724024d32916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ce014-2576-4d5f-a1e2-a34eca35695d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "cl_eps = 100\n",
    "mask_dims = 1\n",
    "n_epochs = 500\n",
    "fig_every = 250\n",
    "\n",
    "d = src_vecs.shape[-1]\n",
    "mask = torch.zeros(d)\n",
    "mask[:mask_dims] = 1\n",
    "\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "rot_mtx, exp_params, cl_loss, actn_loss, acc, emd, row_emd = train_rotation(\n",
    "    src_varbs=src_varbs,\n",
    "    intrv_varbs=intrv_varbs,\n",
    "    classifier=classifier,\n",
    "    trg_vecs=trg_vecs,\n",
    "    src_vecs=src_vecs,\n",
    "    src_classes=src_classes,\n",
    "    intrv_classes=intrv_classes,\n",
    "    mask=mask,\n",
    "    cl_loss_type = \"both\", #\"cos\", \"mse\", \"both\n",
    "    method = \"mean\",  #\"mean\" \"sample\"\n",
    "    calc_loss_in_aligned_basis = False,\n",
    "    detach_cl_vecs = True,\n",
    "    shuffle_empty = False,\n",
    "    incl_extra = False, # Will include the extraneous subspaces in cl loss if True,\n",
    "    lr = lr,\n",
    "    cl_eps=cl_eps,\n",
    "    early_stopping=False,\n",
    "    incl_actn_loss = False,\n",
    "    incl_cl_loss = True,\n",
    "    mtx_type = \"orthog\", # \"orthog\" \"linear\",\n",
    "    print_every = 50,\n",
    "    n_epochs = n_epochs,\n",
    "    fig_every = fig_every,\n",
    "    save_fig=True,\n",
    "    incl_dividers=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad80a5-80a2-4cd0-b6f1-520090e957b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "rot_mtx, exp_params, cl_loss, actn_loss, acc, emd, row_emd = train_rotation(\n",
    "    src_varbs=src_varbs,\n",
    "    intrv_varbs=intrv_varbs,\n",
    "    classifier=classifier,\n",
    "    trg_vecs=trg_vecs,\n",
    "    src_vecs=src_vecs,\n",
    "    src_classes=src_classes,\n",
    "    intrv_classes=intrv_classes,\n",
    "    mask=mask,\n",
    "    cl_loss_type = \"both\", #\"cos\", \"mse\", \"both\n",
    "    method = \"mean\",  #\"mean\" \"sample\"\n",
    "    calc_loss_in_aligned_basis = False,\n",
    "    detach_cl_vecs = True,\n",
    "    shuffle_empty = False,\n",
    "    incl_extra = False, # Will include the extraneous subspaces in cl loss if True,\n",
    "    lr = lr,\n",
    "    cl_eps=cl_eps,\n",
    "    early_stopping=False,\n",
    "    incl_actn_loss = True,\n",
    "    incl_cl_loss = False,\n",
    "    mtx_type = \"orthog\", # \"orthog\" \"linear\",\n",
    "    print_every = 50,\n",
    "    n_epochs = n_epochs,\n",
    "    fig_every = fig_every,\n",
    "    save_fig=True,\n",
    "    incl_dividers=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae37be-060f-449d-b38a-f49514624c74",
   "metadata": {},
   "source": [
    "# Multi Tasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rotation(\n",
    "        rot_mtx,\n",
    "        data,\n",
    "        classifier,\n",
    "        mask=None,\n",
    "        incl_extra=False,\n",
    "        calc_loss_in_aligned_basis=False,\n",
    "        detach_cl_vecs=False,\n",
    "        method=\"mean\",\n",
    "        ylim=[-2.75,1.75],\n",
    "        fig_save_name=None,\n",
    "        **kwargs,\n",
    "):\n",
    "    cl_vecs = get_cl_vectors(\n",
    "        data[\"src_varbs\"], data[\"intrv_varbs\"], data[\"src_vecs\"], method=method\n",
    "    )\n",
    "    if mask is None:\n",
    "        mask = torch.zeros(src_vecs.shape[-1]).cuda()\n",
    "        mask[0] = 1\n",
    "\n",
    "    cl_loss, intrv_vecs = get_cl_loss(\n",
    "        trg=data[\"trg_vecs\"].cuda(),\n",
    "        src=data[\"src_vecs\"].cuda(),\n",
    "        mtx=rot_mtx.cuda(),\n",
    "        mask=mask.cuda(),\n",
    "        cl_vecs=cl_vecs.cuda(),\n",
    "        incl_extra=incl_extra,\n",
    "        calc_loss_in_aligned_basis=calc_loss_in_aligned_basis,\n",
    "        detach_cl_vecs=detach_cl_vecs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        src_vecs = data[\"src_vecs\"]\n",
    "        perm = torch.randperm(len(src_vecs)).long()\n",
    "        emd = compute_emd(src_vecs[perm].cuda(), intrv_vecs.cuda()).item()\n",
    "        extra_mask = torch.zeros_like(mask).cuda()\n",
    "        extra_mask[:2] = 1 # Only using the causal dimensions\n",
    "        row_emd = compute_emd(\n",
    "            intrv_vecs.cuda()*extra_mask, src_vecs.cuda()*extra_mask\n",
    "        ).item()\n",
    "    actn_loss, acc = get_actn_loss(classifier(intrv_vecs.cuda()).cpu(), torch.tensor(data[\"intrv_classes\"]).long().cpu())\n",
    "\n",
    "    print(\"Cl Loss:\", cl_loss.item(), \"ActLoss:\", actn_loss.item(), \"Actn:\", acc.item(), \"EMD:\", emd, \"RowEMD:\", row_emd)\n",
    "    quick_plot(\n",
    "        data[\"src_vecs\"], intrv_vecs.detach(),\n",
    "        data[\"src_classes\"], data[\"intrv_classes\"],\n",
    "        ylim=ylim,\n",
    "        incl_dividers=False,\n",
    "        save_name=fig_save_name,\n",
    "    )\n",
    "    return cl_loss, actn_loss, acc, emd, row_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e290e6d-79d8-4ab2-838e-987ea350094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_divisions = {\n",
    "    'mirror_L',\n",
    "    'mirror_h',\n",
    "    'random_overlap',\n",
    "    'tetris_C',\n",
    "    'tetris_F',\n",
    "    'tetris_L',\n",
    "    'tetris_T',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73cd3f-7fca-47cd-a0b3-a473363e5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1853b-d6c4-490d-8e65-d997d1ced169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c787a11-1f3e-4d70-93d1-09e90bf1e89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data_params = {\n",
    "    \"extra_dims\": 128,\n",
    "    \"dupl_rank\": 0, # duplicates extra dims\n",
    "    \"zero_rank\": 0, # zeros out extra dims in a rotated space\n",
    "    \"cov_strength\": 0, # how much do the extraneous dimensions covary with the x and y values,\n",
    "    \"n_samples_per_class\": 100,\n",
    "    \"mask_dims\": 1,\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    \"lr\": 0.01,\n",
    "    \"patience\": 400,\n",
    "    \"l2\": 0.005,\n",
    "    \"bsize\": 200,\n",
    "    \"patience\": 100,\n",
    "    \"n_epochs\": 1000,\n",
    "    \"drop_p\": 0.5,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"n_layers\": 2, # 1-3 layers\n",
    "    \"pre_batchnorm\": True,\n",
    "    \"batchnorm\": True,\n",
    "    \"ret_best\": True,\n",
    "}\n",
    "\n",
    "\n",
    "divisions = [ \"inner_square\", \"original\" ] # all_divisions\n",
    "mtx_types = [\"orthog\",] # \"linear\"]\n",
    "cl_epses = [1, 10, 50, 100]\n",
    "lrs = [0.05] #, 0.1, 0.01, 0.005]\n",
    "extra_dims_list = [0,16,64,128]\n",
    "incl_extras = [False, True]\n",
    "mask_dims = [1,4,8]\n",
    "n_repeats = 5\n",
    "\n",
    "ylim = [-2.75, 1.75]\n",
    "\n",
    "exp_params = {\n",
    "    \"n_epochs\": 3000,\n",
    "    \"lr\": 0.005,\n",
    "    \"cl_eps\": 1,\n",
    "    \"shuffle_empty\": False,\n",
    "    \"method\": \"mean\",\n",
    "    \"incl_extra\": False,\n",
    "    \"calc_loss_in_aligned_basis\": False,\n",
    "    \"detach_cl_vecs\": True,\n",
    "    \"incl_actn_loss\": True,\n",
    "    \"incl_cl_loss\": True,\n",
    "    \"mtx_type\": \"orthog\",\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "all_keys = {*set(data_params.keys()), *set(mlp_params.keys()), *set(exp_params.keys())}\n",
    "save_keys = [\"incl_extra\", \"extra_dims\", \"lr\", \"incl_actn_loss\", \"incl_cl_loss\", \"cl_eps\", \"mask_dims\"]\n",
    "excl_keys = [key for key in all_keys if key not in save_keys]\n",
    "for incl_extra in incl_extras:\n",
    "    exp_params[\"incl_extra\"] = incl_extra\n",
    "    for repeat in range(n_repeats):\n",
    "        for extra_dims in extra_dims_list:\n",
    "            data_params[\"extra_dims\"] = extra_dims\n",
    "            for lr in lrs:\n",
    "                exp_params[\"lr\"] = lr\n",
    "                for ial,incl_actn_loss in enumerate([False, True]):\n",
    "                    for icl,incl_cl_loss in enumerate([False, True,]):\n",
    "                        if not incl_actn_loss and not incl_cl_loss: continue\n",
    "                        for ice, cl_eps in enumerate(cl_epses):\n",
    "                            if ice > 0 and not incl_cl_loss: continue\n",
    "                            for mask_dim in mask_dims:\n",
    "                                d = data_params[\"extra_dims\"]+2\n",
    "                                if mask_dim*2 > d: continue\n",
    "                                data_params[\"mask_dims\"] = mask_dim\n",
    "                                df_dict = {\n",
    "                                    \"run_id\": [],\n",
    "                                    \"task_num\": [],\n",
    "                                    \"task_division\": [],\n",
    "                                    \"n_samples\": [],\n",
    "                                    \"min_class_count\": [],\n",
    "                                    \"max_class_count\": [],\n",
    "                                    \"mean_class_count\": [],\n",
    "                                    \"class_trn_acc\": [],\n",
    "                                    \"class_val_acc\": [],\n",
    "                                    \"cl_loss\": [],\n",
    "                                    \"actn_loss\": [],\n",
    "                                    \"actn_acc\": [],\n",
    "                                    \"emd\": [],\n",
    "                                    \"row_emd\": [],\n",
    "                                    \"cross_cl_loss\": [],\n",
    "                                    \"cross_actn_loss\": [],\n",
    "                                    \"cross_actn_acc\": [],\n",
    "                                    \"cross_emd\": [],\n",
    "                                    \"cross_row_emd\": [],\n",
    "                                    \"mtx_type\": [],\n",
    "                                }\n",
    "                                \n",
    "                                exp_params[\"incl_cl_loss\"] = incl_cl_loss\n",
    "                                exp_params[\"incl_actn_loss\"] = incl_actn_loss\n",
    "                                exp_params[\"cl_eps\"] = cl_eps\n",
    "                                \n",
    "                                for tdi,task_division in enumerate(divisions):\n",
    "                                    if task_division in excl_divisions:\n",
    "                                        print(\"Skipping\", task_division)\n",
    "                                        continue\n",
    "                                    print(\"Starting Task Division\", task_division)\n",
    "                                    for _ in mtx_types:\n",
    "                                        df_dict[\"task_division\"].append(task_division)\n",
    "                                        df_dict[\"task_division\"].append(task_division)\n",
    "                                        df_dict[\"task_num\"].append(0)\n",
    "                                        df_dict[\"task_num\"].append(1)\n",
    "                                    task1_bools, task2_bools = make_tasks(task_division=task_division, varbs=og_varbs)\n",
    "                                \n",
    "                                    ##########################################################################\n",
    "                                    ### DATA PREP\n",
    "                                    ##########################################################################\n",
    "                                    data_dict = prep_data(\n",
    "                                        og_varbs=og_varbs[task1_bools],\n",
    "                                        samples=samples[task1_bools],\n",
    "                                        **data_params,\n",
    "                                    )\n",
    "                                \n",
    "                                    valids = data_dict[\"valid_intrvs\"]\n",
    "                                    if valids.sum() == 0:\n",
    "                                        print(\"No valid intrvs\")\n",
    "                                        continue\n",
    "                                    task1_data = {\n",
    "                                        \"src_vecs\": data_dict[\"src_vecs\"][valids],\n",
    "                                        \"trg_vecs\": data_dict[\"trg_vecs\"][valids],\n",
    "                                        \"intrv_varbs\": data_dict[\"intrv_varbs\"][valids],\n",
    "                                        \"intrv_classes\": data_dict[\"intrv_classes\"][valids],\n",
    "                                        \"src_varbs\": data_dict[\"src_varbs\"][valids],\n",
    "                                        \"src_classes\": data_dict[\"src_classes\"][valids],\n",
    "                                    }\n",
    "                                    counts = []\n",
    "                                    intrv_classes = data_dict[\"intrv_classes\"]\n",
    "                                    for c in sorted(set(intrv_classes.cpu().tolist())):\n",
    "                                        counts.append((intrv_classes==c).long().sum().item())\n",
    "                                    for _ in mtx_types:\n",
    "                                        df_dict[\"n_samples\"].append(len(intrv_classes))\n",
    "                                        df_dict[\"min_class_count\"].append(np.min(counts))\n",
    "                                        df_dict[\"max_class_count\"].append(np.max(counts))\n",
    "                                        df_dict[\"mean_class_count\"].append(np.mean(counts))\n",
    "                                    print(\"Class Distr 1:\",\n",
    "                                          \"\\n\\tMin:\", df_dict[\"min_class_count\"][-1],\n",
    "                                          \"\\n\\tMax:\", df_dict[\"max_class_count\"][-1],\n",
    "                                          \"\\n\\tMean:\", df_dict[\"mean_class_count\"][-1],\n",
    "                                        )\n",
    "                                \n",
    "                                    data_dict = prep_data(\n",
    "                                        og_varbs=og_varbs[task2_bools],\n",
    "                                        samples=samples[task2_bools],\n",
    "                                        **data_params,\n",
    "                                    )\n",
    "                                \n",
    "                                    valids = data_dict[\"valid_intrvs\"]\n",
    "                                    task2_data = {\n",
    "                                        \"src_vecs\": data_dict[\"src_vecs\"][valids],\n",
    "                                        \"trg_vecs\": data_dict[\"trg_vecs\"][valids],\n",
    "                                        \"intrv_varbs\": data_dict[\"intrv_varbs\"][valids],\n",
    "                                        \"intrv_classes\": data_dict[\"intrv_classes\"][valids],\n",
    "                                        \"src_varbs\": data_dict[\"src_varbs\"][valids],\n",
    "                                        \"src_classes\": data_dict[\"src_classes\"][valids],\n",
    "                                    }\n",
    "                                    counts = []\n",
    "                                    intrv_classes = data_dict[\"intrv_classes\"]\n",
    "                                    for c in sorted(set(intrv_classes.cpu().tolist())):\n",
    "                                        counts.append((intrv_classes==c).long().sum().item())\n",
    "                                    for _ in mtx_types:\n",
    "                                        df_dict[\"n_samples\"].append(len(intrv_classes))\n",
    "                                        df_dict[\"min_class_count\"].append(np.min(counts))\n",
    "                                        df_dict[\"max_class_count\"].append(np.max(counts))\n",
    "                                        df_dict[\"mean_class_count\"].append(np.mean(counts))\n",
    "                                    print(\"Class Distr 2:\",\n",
    "                                          \"\\n\\tMin:\", df_dict[\"min_class_count\"][-1],\n",
    "                                          \"\\n\\tMax:\", df_dict[\"max_class_count\"][-1],\n",
    "                                          \"\\n\\tMean:\", df_dict[\"mean_class_count\"][-1],\n",
    "                                        )\n",
    "                                \n",
    "                                    print(task_division)\n",
    "                                    quick_plot(\n",
    "                                        task1_data[\"src_vecs\"], task2_data[\"src_vecs\"],\n",
    "                                        incl_legend=True,\n",
    "                                        labels=[\"Task1\", \"Task2\"],\n",
    "                                        incl_dividers=False,\n",
    "                                        ylim=ylim,\n",
    "                                    )\n",
    "                            \n",
    "                                    ##########################################################################\n",
    "                                    ### Classifier Training\n",
    "                                    ##########################################################################\n",
    "                                    print(\"Training Classifier1\")\n",
    "                                    classifier1, trn_acc, val_acc = train_classifier(\n",
    "                                        task1_data[\"src_vecs\"],\n",
    "                                        task1_data[\"src_classes\"],\n",
    "                                        **mlp_params,\n",
    "                                        verbose=False,\n",
    "                                    )\n",
    "                                    classifier1.cuda()\n",
    "                                    classifier1.eval()\n",
    "                                    for p in classifier1.parameters():\n",
    "                                        p.requires_grad = False\n",
    "                                    for _ in mtx_types:\n",
    "                                        df_dict[\"class_trn_acc\"].append(trn_acc)\n",
    "                                        df_dict[\"class_val_acc\"].append(val_acc)\n",
    "                                    \n",
    "                                    print(\"Training Classifier2\")\n",
    "                                    classifier2, trn_acc, val_acc = train_classifier(\n",
    "                                        task2_data[\"src_vecs\"],\n",
    "                                        task2_data[\"src_classes\"],\n",
    "                                        **mlp_params,\n",
    "                                        verbose=False,\n",
    "                                    )\n",
    "                                    classifier2.cuda()\n",
    "                                    classifier2.eval()\n",
    "                                    for p in classifier2.parameters():\n",
    "                                        p.requires_grad = False\n",
    "                                    for _ in mtx_types:\n",
    "                                        df_dict[\"class_trn_acc\"].append(trn_acc)\n",
    "                                        df_dict[\"class_val_acc\"].append(val_acc)\n",
    "                                    \n",
    "                                    mask = data_dict[\"mask\"]\n",
    "                                \n",
    "                                    ##########################################################################\n",
    "                                    ### Rotation Matrix Training\n",
    "                                    ##########################################################################\n",
    "                                    for mtx_type in mtx_types:\n",
    "                                        print(\"--------------------\")\n",
    "                                        print(\"Performing New Training\")\n",
    "                                        for k in sorted(save_keys):\n",
    "                                            if k in exp_params: v = exp_params[k]\n",
    "                                            elif k in data_params: v = data_params[k]\n",
    "                                            elif k in mlp_params: v = mlp_params[k]\n",
    "                                            print(k,v)\n",
    "                                        print()\n",
    "                                            \n",
    "                                        exp_params[\"mtx_type\"] = mtx_type\n",
    "                                        run_id = time.time()\n",
    "                                        df_dict[\"run_id\"].append(run_id)\n",
    "                                        df_dict[\"run_id\"].append(run_id)\n",
    "                                        df_dict[\"mtx_type\"].append(mtx_type)\n",
    "                                        df_dict[\"mtx_type\"].append(mtx_type)\n",
    "                                        print(\"Training Task1 Matrix\", mtx_type)\n",
    "                                        task1_rot_mtx, _, cl_loss, actn_loss, acc, emd, row_emd = train_rotation(\n",
    "                                            **task1_data,\n",
    "                                            classifier=classifier1,\n",
    "                                            mask=mask,\n",
    "                                            **exp_params,\n",
    "                                            print_every=200,\n",
    "                                            fig_every=np.inf,\n",
    "                                            early_stopping=True,\n",
    "                                        )\n",
    "                                        print(\"End Task1 Training\")\n",
    "                                        print()\n",
    "                                        df_dict[\"cl_loss\"].append(float(cl_loss))\n",
    "                                        df_dict[\"actn_loss\"].append(float(actn_loss))\n",
    "                                        df_dict[\"actn_acc\"].append(float(acc))\n",
    "                                        df_dict[\"emd\"].append(float(emd))\n",
    "                                        df_dict[\"row_emd\"].append(float(row_emd))\n",
    "                                        \n",
    "                                    \n",
    "                                        print(\"Training Task2 Matrix\")\n",
    "                                        task2_rot_mtx, _, cl_loss, actn_loss, acc, emd, row_emd = train_rotation(\n",
    "                                            **task2_data,\n",
    "                                            classifier=classifier2,\n",
    "                                            mask=mask,\n",
    "                                            **exp_params,\n",
    "                                            print_every=200,\n",
    "                                            fig_every=np.inf,\n",
    "                                            early_stopping=True,\n",
    "                                        )\n",
    "                                        print(\"End Task2 Training\")\n",
    "                                        print()\n",
    "                                        df_dict[\"cl_loss\"].append(float(cl_loss))\n",
    "                                        df_dict[\"actn_loss\"].append(float(actn_loss))\n",
    "                                        df_dict[\"actn_acc\"].append(float(acc))\n",
    "                                        df_dict[\"emd\"].append(float(emd))\n",
    "                                        df_dict[\"row_emd\"].append(float(row_emd))\n",
    "                                    \n",
    "                                    \n",
    "                                        ##########################################################################\n",
    "                                        ### Testing\n",
    "                                        ##########################################################################\n",
    "                                        save_params = {**data_params, **exp_params}\n",
    "                                        for k in list(save_params.keys()):\n",
    "                                            if k in excl_keys: del save_params[k]\n",
    "                                        save_params[\"task\"] = 1\n",
    "                                        save_name = os.path.join(\"figs/\",get_plot_save_name(save_params))\n",
    "                                        print(\"Testing Task1 Matrix on Task2 Data\")\n",
    "                                        cl_loss, actn_loss, acc, emd, row_emd = test_rotation(\n",
    "                                            task1_rot_mtx,\n",
    "                                            task2_data,\n",
    "                                            classifier=classifier2,\n",
    "                                            mask=mask,\n",
    "                                            **exp_params,\n",
    "                                            ylim=ylim,\n",
    "                                            fig_save_name=save_name,\n",
    "                                        )\n",
    "                                        df_dict[\"cross_cl_loss\"].append(float(cl_loss))\n",
    "                                        df_dict[\"cross_actn_loss\"].append(float(actn_loss))\n",
    "                                        df_dict[\"cross_actn_acc\"].append(float(acc))\n",
    "                                        df_dict[\"cross_emd\"].append(float(emd))\n",
    "                                        df_dict[\"cross_row_emd\"].append(float(row_emd))\n",
    "                                        \n",
    "                                        print(\"Testing Task2 Matrix on Task1 Data\")\n",
    "                                        save_params[\"task\"] = 2\n",
    "                                        save_name = os.path.join(\"figs/\",get_plot_save_name(save_params))\n",
    "                                        cl_loss, actn_loss, acc, emd, row_emd = test_rotation(\n",
    "                                            task2_rot_mtx,\n",
    "                                            task1_data,\n",
    "                                            classifier=classifier1,\n",
    "                                            mask=mask,\n",
    "                                            **exp_params,\n",
    "                                            ylim=ylim,\n",
    "                                            fig_save_name=save_name,\n",
    "                                        )\n",
    "                                        df_dict[\"cross_cl_loss\"].append(float(cl_loss))\n",
    "                                        df_dict[\"cross_actn_loss\"].append(float(actn_loss))\n",
    "                                        df_dict[\"cross_actn_acc\"].append(float(acc))\n",
    "                                        df_dict[\"cross_emd\"].append(float(emd))\n",
    "                                        df_dict[\"cross_row_emd\"].append(float(row_emd))\n",
    "                                        \n",
    "                                        print(\"-\"*100)\n",
    "                                        print()\n",
    "                                        print()\n",
    "                                        print()\n",
    "                                        print()\n",
    "                                        print()\n",
    "                                df = pd.DataFrame(df_dict)\n",
    "                                for k in exp_params:\n",
    "                                    if k!=\"mtx_type\":\n",
    "                                        df[k] = exp_params[k]\n",
    "                                for k in data_params:\n",
    "                                    df[k] = data_params[k]\n",
    "                                dfs.append(df)\n",
    "                    full_df = pd.concat(dfs)\n",
    "                    full_df.to_csv(\"csvs/cl_ablations.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082a474-8e9a-4434-8015-1759a0f82c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fdabb-dbc5-46fe-8283-76727c470cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"csvs/cl_ablations.csv\")\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c39a63-fd9e-409c-bc99-301f4248419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"run_id\",\n",
    "    \"task_num\", \"task_division\", \"mtx_type\", \"incl_cl_loss\", \"incl_actn_loss\", \"cl_eps\",\n",
    "    \"min_class_count\", \"max_class_count\", \"mean_class_count\",\n",
    "    \"actn_acc\", \"cross_actn_acc\",\n",
    "    \"emd\", \"cross_emd\",\n",
    "    \"row_emd\", \"cross_row_emd\",\n",
    "]\n",
    "# exp_keys = list(exp_params.keys())\n",
    "# df = pd.merge(left=full_df, right=counts_df, on=[\"task_num\", \"task_division\"]+exp_keys) \n",
    "# df.sort_values(by=[\"task_division\", \"task_num\", \"cross_actn_acc\"], ascending=False)[cols]\n",
    "full_df.sort_values(by=[\"task_division\", \"task_num\", \"cross_actn_acc\"], ascending=False)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8287a8b-7c04-46df-8ba8-8ec7e5c92183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%m-%d-%Y_%HH%MM')\n",
    "print(now)\n",
    "full_df.to_csv(f\"csvs/inner_square_ablations_{now}.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53971128-81b1-4f5d-a42f-a9f9dfb5e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_map = {\n",
    "    \"Original\": 0,\n",
    "    \"Dense\": 1,\n",
    "    \"Sparse\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4b793-ed7f-44be-99b1-73ea5c9cef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"train_type\"] = full_df.apply(\n",
    "    lambda x: f\"{x.mtx_type}{x.incl_cl_loss*'_cl'}{x.incl_actn_loss*'_actn'}_{x.cl_eps}\",\n",
    "    axis=1)\n",
    "full_df[\"task_spacing\"] = \"Original\"\n",
    "full_df.loc[(full_df[\"task_num\"]==0)&(full_df[\"task_division\"]==\"inner_square\"), \"task_spacing\"] = \"Sparse\"\n",
    "full_df.loc[(full_df[\"task_num\"]==1)&(full_df[\"task_division\"]==\"inner_square\"), \"task_spacing\"] = \"Dense\"\n",
    "full_df[\"spacing_order\"] = full_df.apply(lambda x: sort_map[x.task_spacing], axis=1)\n",
    "full_df.loc[~full_df[\"incl_cl_loss\"]&full_df[\"incl_actn_loss\"], \"cl_eps\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5686222-7ec3-4b57-886a-8741f59c4c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_df.loc[full_df[\"incl_actn_loss\"]&~full_df[\"incl_cl_loss\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb1c52-c286-4268-a9d8-56905a37f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 6\n",
    "hue = \"task_spacing\"\n",
    "incl_filters = {\n",
    "    #\"mtx_type\": [\"orthog\"],\n",
    "    \"incl_extra\": [False],\n",
    "}\n",
    "excl_filters = {\n",
    "    \"task_division\": [*excl_divisions]+[\"original\"],\n",
    "}\n",
    "p = [sns.color_palette(\"pastel\")[i] for i in [-3,0,4,6,3,2,0,6,7,-1]]\n",
    "\n",
    "plot_df = full_df.copy()\n",
    "for filt,vals in incl_filters.items():\n",
    "    plot_df = plot_df.loc[plot_df[filt].isin(vals)]\n",
    "for filt,vals in excl_filters.items():\n",
    "    plot_df = plot_df.loc[~plot_df[filt].isin(vals)]\n",
    "\n",
    "best_ttypes = set(plot_df.sort_values(by=[\"task_num\",\"cross_actn_acc\"], ascending=[True,False]).head(top_k)[\"train_type\"])\n",
    "plot_df = plot_df.loc[plot_df[\"train_type\"].isin(best_ttypes)]\n",
    "\n",
    "rot = 35\n",
    "fig = plt.figure()\n",
    "sns.barplot(x=\"train_type\", y=\"actn_acc\", hue=hue, data=plot_df, palette=p)\n",
    "plt.xticks(rotation=rot)\n",
    "plt.title(\"Within Task Accuracy\")\n",
    "plt.legend(loc=\"lower left\", title=\"Training Class Spacing\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.barplot(x=\"train_type\", y=\"cross_actn_acc\", hue=hue, data=plot_df, palette=p)\n",
    "plt.xticks(rotation=rot)\n",
    "plt.title(\"Cross Task Accuracy\")\n",
    "plt.legend(loc=\"lower left\", title=\"Training Class Spacing\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.barplot(x=\"train_type\", y=\"emd\", hue=hue, data=plot_df, palette=p)\n",
    "plt.xticks(rotation=rot)\n",
    "plt.title(\"EMD\")\n",
    "plt.legend(loc=\"lower left\", title=\"Training Class Spacing\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.barplot(x=\"train_type\", y=\"row_emd\", hue=hue, data=plot_df, palette=p)\n",
    "plt.xticks(rotation=rot)\n",
    "plt.title(\"Row EMD\")\n",
    "plt.legend(loc=\"lower left\", title=\"Training Class Spacing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e686b832-c9ff-46d2-a7f4-70a8cc250940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df = pd.read_csv(\"csvs/cl_sweep_128_and_16_dims.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a5321-582a-4a38-a89d-f8c19d8f3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"pastel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03165e99-2f86-4a44-afaf-172e74fc4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_order = [\n",
    "    #-3,0,4,4,6,3,2,0,6,7,-1\n",
    "    -3,2,4,4,6,3,2,0,6,7,-1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474dcbe-a4c3-4e8a-b274-d58623bf63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(full_df[\"extra_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970b8f7-51dd-4b49-8246-b28eae67fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(full_df[\"mask_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97094474-b8e2-4f0f-8d4d-83dd2df9efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(full_df[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753e53a-99f9-4885-9c98-e3af460ad2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d373f-3b11-4e3e-9943-906207ddeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dims = 64\n",
    "incl_filters = {\n",
    "    #\"mtx_type\": [\"orthog\"],\n",
    "    \"extra_dims\": [extra_dims], # 16,],\n",
    "    \"incl_extra\": [False],\n",
    "    #\"prop_rank\": [1,],\n",
    "    \"cov_strength\": [0], #,1,], # how much do the extraneous dimensions covary with the x and y values,\n",
    "    \"mask_dims\": [1,],\n",
    "    \"lr\": [0.05],\n",
    "    #\"cl_eps\": [50],\n",
    "    #\"task_spacing\": [\"Sparse\"],\n",
    "    #\"task_division\": [\"original\"],\n",
    "}\n",
    "excl_filters = {\n",
    "    \"task_division\": excl_divisions,\n",
    "    #\"task_spacing\": [\"Dense\"],\n",
    "}\n",
    "class_acc_threshold = 0\n",
    "\n",
    "rot = 35\n",
    "ylim = [0.58,1.02]\n",
    "x = \"cl_eps\"\n",
    "y = \"cross_actn_acc\"\n",
    "hue = \"task_spacing\"\n",
    "mtx_type = \"orthog\"\n",
    "leg_title = \"Train Task\" # \" \".join([h.capitalize() for h in hue.split(\"_\")])\n",
    "p = [sns.color_palette(\"pastel\")[i] for i in color_order]\n",
    "labelsize = 25\n",
    "ticksize = 28\n",
    "fontsize = 30\n",
    "titlesize = 25\n",
    "legendsize = 20\n",
    "linewidth = 4\n",
    "err_alpha = 0.3\n",
    "yticks = [0.6, 0.8, 1.0]\n",
    "\n",
    "\n",
    "\n",
    "plot_df = full_df.loc[full_df[\"mtx_type\"]==mtx_type].copy()\n",
    "plot_df = plot_df.loc[plot_df[\"class_val_acc\"]>class_acc_threshold]\n",
    "for filt,vals in incl_filters.items():\n",
    "    plot_df = plot_df.loc[plot_df[filt].isin(vals)]\n",
    "for filt,vals in excl_filters.items():\n",
    "    plot_df = plot_df.loc[~plot_df[filt].isin(vals)]\n",
    "plot_df = plot_df.copy()\n",
    "#combo_df = plot_df.loc[plot_df[\"incl_actn_loss\"]&~plot_df[\"incl_cl_loss\"]].copy()\n",
    "#combo_df[\"cl_eps\"] = 0\n",
    "#combo_df[\"incl_cl_loss\"] = True\n",
    "#cl_df = combo_df.copy()\n",
    "#cl_df[\"incl_actn_loss\"] = False\n",
    "#plot_df = pd.concat([ plot_df, combo_df, cl_df, ])\n",
    "\n",
    "bools = plot_df[\"incl_actn_loss\"]&~plot_df[\"incl_cl_loss\"]\n",
    "bloss_acc = dict(plot_df.loc[bools].groupby(hue)[y].mean())\n",
    "bloss_err = dict(plot_df.loc[bools].groupby(hue)[y].sem())\n",
    "bloss_emd = dict(plot_df.loc[bools].groupby(hue)[\"cross_row_emd\"].mean())\n",
    "bloss_emr = dict(plot_df.loc[bools].groupby(hue)[\"cross_row_emd\"].sem())\n",
    "print(\"Behavior Only Accuracy\", bloss_acc, \"+/-\", bloss_err)\n",
    "print(\"Behavior Only EMD\", bloss_emd, \"+/-\", bloss_emr)\n",
    "\n",
    "fig,axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "ax = axes[0]\n",
    "plt.sca(ax)\n",
    "temp_df = plot_df.loc[plot_df[\"incl_cl_loss\"]&~plot_df[\"incl_actn_loss\"]].sort_values(by=\"spacing_order\")\n",
    "sns.barplot(x=x, y=y, hue=hue, data=temp_df, ax=ax, palette=p)\n",
    "plt.title(\"CL Loss Only\", fontsize=titlesize)\n",
    "plt.ylim(ylim)\n",
    "#xlabel = \" \".join([lab.capitalize() for lab in x.split(\"_\")])\n",
    "#plt.xlabel(xlabel, fontsize=labelsize)\n",
    "plt.xlabel(\"CL Epsilon\", fontsize=labelsize)\n",
    "plt.ylabel(\"Cross Task IIA\", fontsize=labelsize)\n",
    "#xticks = list(range(len(set(plot_df[x]))))\n",
    "#xtick_labels = [\"Behavior\", *sorted(list(set(plot_df[x])))[1:]]\n",
    "#plt.xticks(xticks, xtick_labels, rotation=rot, fontsize=ticksize)\n",
    "plt.xticks(fontsize=ticksize)\n",
    "plt.yticks(yticks,fontsize=ticksize)\n",
    "plt.legend(loc=\"lower left\", title=leg_title, fontsize=legendsize, title_fontsize=legendsize)\n",
    "\n",
    "# Baseline\n",
    "xs = list(range(len(set(plot_df[x]))))\n",
    "xs = [np.min(xs)-1] + xs + [np.max(xs)]\n",
    "xs = np.asarray(xs)\n",
    "for i,k in enumerate(bloss_acc):\n",
    "    acc = bloss_acc[k]\n",
    "    err = bloss_err[k]\n",
    "    color = p[sort_map[k]]\n",
    "    ys = np.asarray([acc for _ in xs])\n",
    "    plt.plot(xs, ys, \"--\", color=color, alpha=1, linewidth=linewidth)\n",
    "    plt.fill_between(xs, ys-err, ys+err, alpha=err_alpha, color=color, )\n",
    "\n",
    "ax = axes[1]\n",
    "plt.sca(ax)\n",
    "temp_df = plot_df.loc[plot_df[\"incl_cl_loss\"]&plot_df[\"incl_actn_loss\"]].sort_values(by=\"spacing_order\")\n",
    "sns.barplot(x=x, y=y, hue=hue, data=temp_df, ax=ax, palette=p)\n",
    "plt.title(\"DAS + CL Loss\", fontsize=titlesize)\n",
    "plt.ylim(ylim)\n",
    "#xlabel = \" \".join([lab.capitalize() for lab in x.split(\"_\")])\n",
    "#plt.xlabel(xlabel, fontsize=labelsize)\n",
    "plt.xlabel(\"CL Epsilon\", fontsize=labelsize)\n",
    "plt.ylabel(\"Cross Task IIA\", fontsize=labelsize)\n",
    "#plt.xticks(xticks, rotation=rot, fontsize=ticksize)\n",
    "plt.xticks(fontsize=ticksize)\n",
    "plt.yticks(yticks, fontsize=ticksize)\n",
    "plt.legend(loc=\"lower left\", title=leg_title, fontsize=legendsize, title_fontsize=legendsize).set_visible(False)\n",
    "\n",
    "# Baseline\n",
    "xs = list(range(len(set(plot_df[x]))))\n",
    "xs = [np.min(xs)-1] + xs + [np.max(xs)]\n",
    "xs = np.asarray(xs)\n",
    "for i,k in enumerate(bloss_acc):\n",
    "    acc = bloss_acc[k]\n",
    "    err = bloss_err[k]\n",
    "    color = p[sort_map[k]]\n",
    "    ys = np.asarray([acc for _ in xs])\n",
    "    plt.plot(xs, ys, \"--\", color=color, alpha=1, linewidth=linewidth)\n",
    "    plt.fill_between(xs, ys-err, ys+err, alpha=err_alpha, color=color, )\n",
    "\n",
    "ax = axes[2]\n",
    "plt.sca(ax)\n",
    "temp_df = plot_df.loc[plot_df[\"incl_cl_loss\"]&plot_df[\"incl_actn_loss\"]].sort_values(by=\"spacing_order\")\n",
    "sns.barplot(x=x, y=\"cross_row_emd\", hue=hue, data=temp_df, ax=ax, palette=p)\n",
    "plt.title(\"DAS + CL EMD\", fontsize=titlesize)\n",
    "plt.xlabel(\"CL Epsilon\", fontsize=labelsize)\n",
    "plt.ylabel(\"Cross Task Causal EMD\", fontsize=labelsize)\n",
    "#plt.ylim(ylim)\n",
    "#xlabel = \" \".join([lab.capitalize() for lab in x.split(\"_\")])\n",
    "#plt.xlabel(xlabel, fontsize=labelsize)\n",
    "#plt.xticks(xticks, rotation=rot, fontsize=ticksize)\n",
    "plt.xticks(fontsize=ticksize)\n",
    "plt.yticks(fontsize=ticksize)\n",
    "plt.legend(loc=\"lower left\", title=leg_title, fontsize=legendsize, title_fontsize=legendsize).set_visible(False)\n",
    "\n",
    "# Baseline\n",
    "xs = list(range(len(set(plot_df[x]))))\n",
    "xs = [np.min(xs)-1] + xs + [np.max(xs)]\n",
    "xs = np.asarray(xs)\n",
    "for i,k in enumerate(bloss_acc):\n",
    "    emd = bloss_emd[k]\n",
    "    err = bloss_emr[k]\n",
    "    color = p[sort_map[k]]\n",
    "    ys = np.asarray([emd for _ in xs])\n",
    "    plt.plot(xs, ys, \"--\", color=color, alpha=1, linewidth=linewidth)\n",
    "    plt.fill_between(xs, ys-err, ys+err, alpha=err_alpha, color=color, )\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f\"figs/ood_inner_square_{extra_dims}d.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31acace-8858-4b22-8433-d05a6baa17c4",
   "metadata": {},
   "source": [
    "### EMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d086b-e296-4eaf-9297-6e49920c2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "incl_filters = {\n",
    "    #\"mtx_type\": [\"orthog\"],\n",
    "    \"incl_extra\": [False], # refers to whether the CL loss is applied to only the masked dimensions or all dimensions\n",
    "    \"extra_dims\": [64], # 16,],\n",
    "    \"mask_dims\": [1,],\n",
    "    \"lr\": [0.05],\n",
    "    #\"prop_rank\": [1,],\n",
    "    #\"cov_strength\": [0], #,1,], # how much do the extraneous dimensions covary with the x and y values,\n",
    "    #\"cl_eps\": [50],\n",
    "    #\"task_spacing\": [\"Sparse\"],\n",
    "}\n",
    "excl_filters = {\n",
    "    \"task_division\": excl_divisions,\n",
    "    #\"task_spacing\": [\"Dense\"],\n",
    "}\n",
    "class_acc_threshold = 0 # filters out MLPs that failed to solve the classification task\n",
    "\n",
    "groups = [\"cl_eps\", \"task_spacing\", \"spacing_order\"]\n",
    "metrics = [\"emd\", \"row_emd\", \"cross_emd\", \"cross_row_emd\", \"actn_acc\", \"cross_actn_acc\",]\n",
    "mtx_type = \"orthog\"\n",
    "\n",
    "plot_df = full_df.loc[full_df[\"mtx_type\"]==mtx_type].copy()\n",
    "plot_df = plot_df.loc[(plot_df[\"class_val_acc\"]>class_acc_threshold)]\n",
    "for filt,vals in incl_filters.items():\n",
    "    plot_df = plot_df.loc[plot_df[filt].isin(vals)]\n",
    "for filt,vals in excl_filters.items():\n",
    "    plot_df = plot_df.loc[~plot_df[filt].isin(vals)]\n",
    "plot_df = plot_df.copy()\n",
    "\n",
    "temp = plot_df.groupby(groups)[metrics]\\\n",
    "    .agg([\"mean\", \"sem\"]).reset_index()\n",
    "columns = []\n",
    "for col in temp.columns:\n",
    "    if type(col)==tuple:\n",
    "        columns.append(f\"{col[0]} {col[1]}\".replace(\" mean\", \"\").strip())\n",
    "    else:\n",
    "        columns.append(col.strip())\n",
    "temp.columns = columns\n",
    "#temp.sort_values(by=[\"spacing_order\", \"cl_eps\"], ascending=False)\n",
    "temp.sort_values(by=[\"spacing_order\", \"cl_eps\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe86853-e583-4fc7-9a93-61834552d34e",
   "metadata": {},
   "source": [
    "### Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27a127-589a-4508-8498-e60d65339b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(full_df[\"cov_strength\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd320d-dfc6-4969-879d-3d04badf67fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf1b67-11ff-4da8-bd70-4af60c40fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "incl_filters = {\n",
    "    #\"mtx_type\": [\"orthog\"],\n",
    "    \"incl_extra\": [False], # refers to whether the CL loss is applied to only the masked dimensions or all dimensions\n",
    "    #\"extra_dims\": [64], # 16,],\n",
    "    #\"mask_dims\": [1,],\n",
    "    \"lr\": [ 0.05 ],\n",
    "    #\"prop_rank\": [1,],\n",
    "    #\"cov_strength\": [0], #,1,], # how much do the extraneous dimensions covary with the x and y values,\n",
    "    #\"cl_eps\": [50],\n",
    "    #\"task_spacing\": [\"Sparse\"],\n",
    "}\n",
    "excl_filters = {\n",
    "    \"task_division\": excl_divisions,\n",
    "    #\"task_spacing\": [\"Dense\"],\n",
    "}\n",
    "class_acc_threshold = 0.99 # filters out MLPs that failed to solve the classification task\n",
    "\n",
    "rot = 35\n",
    "x = \"cl_eps\"\n",
    "y = \"cross_actn_acc\"\n",
    "hue = \"task_spacing\"\n",
    "col = \"mask_dims\"\n",
    "row = \"extra_dims\"\n",
    "mtx_type = \"orthog\"\n",
    "leg_title = \"Train Task\" # \" \".join([h.capitalize() for h in hue.split(\"_\")])\n",
    "p = [sns.color_palette(\"pastel\")[i] for i in color_order]\n",
    "labelsize = 25\n",
    "ticksize = 28\n",
    "fontsize = 30\n",
    "titlesize = 20\n",
    "legendsize = 20\n",
    "linewidth = 4\n",
    "\n",
    "ylabel = \"Cross Task IIA\" if \"cross\" in y else \"Trained Task IIA\" \n",
    "ylim = [0.58,1.02]\n",
    "yticks = [0.6, 0.8, 1.0]\n",
    "if \"acc\" not in y:\n",
    "    if \"row\" in y:\n",
    "        ylabel = ylabel.replace(\"Task IIA\", \"Row EMD\")\n",
    "        yticks = [0,0.025,0.05, 0.075, 0.1, 0.125, 0.15]\n",
    "        ylim = [0,0.155]\n",
    "    else:\n",
    "        ylabel = ylabel.replace(\"Task IIA\", \"EMD\")\n",
    "        yticks = None\n",
    "        ylim = None\n",
    "print(y)\n",
    "iia_threshold = 0\n",
    "\n",
    "\n",
    "plot_df = full_df.loc[full_df[\"mtx_type\"]==mtx_type].copy()\n",
    "plot_df = plot_df.loc[(plot_df[\"class_val_acc\"]>class_acc_threshold)&(plot_df[\"actn_acc\"]>iia_threshold)]\n",
    "for filt,vals in incl_filters.items():\n",
    "    plot_df = plot_df.loc[plot_df[filt].isin(vals)]\n",
    "for filt,vals in excl_filters.items():\n",
    "    plot_df = plot_df.loc[~plot_df[filt].isin(vals)]\n",
    "plot_df = plot_df.copy()\n",
    "#combo_df = plot_df.loc[plot_df[\"incl_actn_loss\"]&~plot_df[\"incl_cl_loss\"]].copy()\n",
    "#combo_df[\"cl_eps\"] = 0\n",
    "#combo_df[\"incl_cl_loss\"] = True\n",
    "#cl_df = combo_df.copy()\n",
    "#cl_df[\"incl_actn_loss\"] = False\n",
    "#plot_df = pd.concat([ plot_df, combo_df, cl_df, ])\n",
    "\n",
    "bloss_acc = dict(plot_df.loc[plot_df[\"incl_actn_loss\"]&~plot_df[\"incl_cl_loss\"]].groupby(hue)[y].mean())\n",
    "bloss_err = dict(plot_df.loc[plot_df[\"incl_actn_loss\"]&~plot_df[\"incl_cl_loss\"]].groupby(hue)[y].sem())\n",
    "print(\"Behavior Only Accuracy\", bloss_acc, \"+/-\", bloss_err)\n",
    "\n",
    "#fig,axes = plt.subplots(1,2, figsize=(10,5))\n",
    "temp_df = plot_df.loc[plot_df[\"incl_actn_loss\"]].sort_values(by=[\"spacing_order\",x], ascending=[True,False])\n",
    "temp_df[\"emd\"] = temp_df[\"emd\"]/(temp_df[\"extra_dims\"]+1)\n",
    "temp_df[\"cross_emd\"] = temp_df[\"cross_emd\"]/(temp_df[\"extra_dims\"]+1)\n",
    "g = sns.catplot(\n",
    "    x=x, y=y,\n",
    "    hue=hue,\n",
    "    col=col,\n",
    "    row=row,\n",
    "    data=temp_df,\n",
    "    ax=ax, palette=p, kind=\"bar\",\n",
    ")\n",
    "for i,(_, ax) in enumerate(g.axes_dict.items()):\n",
    "    plt.sca(ax)\n",
    "    #plt.title(\"DAS + CL Loss\", fontsize=titlesize)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    #xlabel = \" \".join([lab.capitalize() for lab in x.split(\"_\")])\n",
    "    #plt.xlabel(xlabel, fontsize=labelsize)\n",
    "    plt.xlabel(\"CL Epsilon\", fontsize=labelsize)\n",
    "    plt.ylabel(ylabel, fontsize=labelsize)\n",
    "    #plt.xticks(xticks, rotation=rot, fontsize=ticksize)\n",
    "    plt.xticks(fontsize=ticksize)\n",
    "    if yticks is None:\n",
    "        plt.yticks(fontsize=ticksize)\n",
    "    else:\n",
    "        plt.yticks(yticks, fontsize=ticksize)\n",
    "    plt.title(\n",
    "        ax.get_title(),\n",
    "        fontsize=titlesize\n",
    "    )\n",
    "    if i==9:\n",
    "        plt.legend(\n",
    "            title=leg_title,\n",
    "            fontsize=legendsize,\n",
    "            title_fontsize=legendsize\n",
    "        )\n",
    "    else:\n",
    "        plt.legend(\n",
    "            loc=\"lower left\",\n",
    "            title=leg_title,\n",
    "            fontsize=legendsize,\n",
    "            title_fontsize=legendsize\n",
    "        ).set_visible(False)\n",
    "\n",
    "## Baseline\n",
    "#xs = list(range(len(set(plot_df[x]))))\n",
    "#xs = [np.min(xs)-1] + xs + [np.max(xs)]\n",
    "#xs = np.asarray(xs)\n",
    "#for i,k in enumerate(bloss_acc):\n",
    "#    acc = bloss_acc[k]\n",
    "#    err = bloss_err[k]\n",
    "#    color = p[i]\n",
    "#    ys = np.asarray([acc for _ in xs])\n",
    "#    plt.plot(xs, ys, \"--\", color=color, alpha=1, linewidth=linewidth)\n",
    "#    #plt.fill_between(xs, ys-err, ys+err, alpha=0.2, color=color, )\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f\"figs/ood_ablations_c{col}_r{row}_{y}.png\", dpi=600, bbox_inches=\"tight\")\n",
    "#plt.savefig(f\"figs/ood_ablations_c{col}_r{row}_{y}.pdf\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e00247-33f1-400f-83a5-5841d9b936b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "temp_df = plot_df.loc[plot_df[\"incl_cl_loss\"]&plot_df[\"incl_actn_loss\"]].sort_values(by=x, ascending=False)\n",
    "ax = plt.gca()\n",
    "sns.barplot(x=x, y=y, hue=hue, data=temp_df, ax=ax, palette=p)\n",
    "plt.title(\"DAS + CL Loss\", fontsize=titlesize)\n",
    "plt.ylim(ylim)\n",
    "#xlabel = \" \".join([lab.capitalize() for lab in x.split(\"_\")])\n",
    "#plt.xlabel(xlabel, fontsize=labelsize)\n",
    "plt.xlabel(\"CL Epsilon\", fontsize=labelsize)\n",
    "plt.ylabel(\"Cross Task IIA\", fontsize=labelsize)\n",
    "#plt.xticks(xticks, rotation=rot, fontsize=ticksize)\n",
    "plt.xticks(fontsize=ticksize)\n",
    "plt.yticks(yticks, fontsize=ticksize)\n",
    "plt.legend(\n",
    "    loc=\"lower left\",\n",
    "    bbox_to_anchor=(1,0),\n",
    "    title=leg_title,\n",
    "    fontsize=legendsize,\n",
    "    title_fontsize=legendsize\n",
    ")\n",
    "plt.savefig(\"figs/ood_legend.png\", dpi=600,bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa6619-4340-45c7-85d9-63e687db20be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
