{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b0887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import gc\n",
    "import dataclasses\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model_and_tokenizer(gpu_num, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "    device = torch.device(f\"cuda:{gpu_num}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        #cache_dir=\"/data2/sjeromeh/cache/pretrained_models\",\n",
    "        dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        #cache_dir=\"/data2/sjeromeh/cache/pretrained_models\"\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def clear_hooks(input_hooks):\n",
    "    for hook in input_hooks:\n",
    "        hook.remove()\n",
    "    input_hooks = []\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982a355-c18b-4799-820c-bf7c9d1da3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from huggingface_hub import login\n",
    "except:\n",
    "    !pip install huggingface_hub\n",
    "    from huggingface_hub import login\n",
    "# either argue your huggingface token, or put it in a file\n",
    "# named \"your_token.txt\" in the working directory\n",
    "with open(\"your_token.txt\", \"r\") as f:\n",
    "    your_token = f.read().strip()\n",
    "login(token=your_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e54882",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb74a8b-2637-47f0-9923-c3c263c61a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose model here\n",
    "# Options are keys of model_names dict below\n",
    "model_alias = \"llama-3-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f8031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = {\n",
    "    \"pythia-7b\": \"EleutherAI/pythia-6.9b\",\n",
    "    \"llama-3-1\": \"meta-llama/Llama-3.1-8B\",\n",
    "    \"llama-3-instruct\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"llama-2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "}\n",
    "space_token = {\n",
    "    \"pythia-7b\": \"Ġ\",\n",
    "    \"llama-3-1\": \"Ġ\",\n",
    "    \"llama-2-7b-chat\": \"▁\",\n",
    "    \"llama-3-instruct\":\" \",\n",
    "}[model_alias]\n",
    "model_name = model_names[model_alias]\n",
    "model, tokenizer = load_model_and_tokenizer(0, model_name=model_name)\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4b570",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3bd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv(\"data/countries.csv\")\n",
    "names_df = pd.read_csv(\"data/names.csv\", index_col=0)\n",
    "\n",
    "countries_to_capitals = countries_df.set_index(\"country\")[\"capital\"].to_dict()\n",
    "names = names_df[\"name\"].tolist()\n",
    "\n",
    "# Keep only single token countries and names\n",
    "countries_to_capitals = {k: v for k, v in countries_to_capitals.items() if len(tokenizer.encode(f\" {k}\", add_special_tokens=False)) == 1}\n",
    "names = [n for n in names if len(tokenizer.encode(n, add_special_tokens=False)) == 1]\n",
    "\n",
    "print(f\"Number of countries: {len(countries_to_capitals)}, Sample: {list(countries_to_capitals.items())[:5]}\")\n",
    "print(f\"Number of names: {len(names)}, Sample: {names[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54a8c5",
   "metadata": {},
   "source": [
    "# Prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d3f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class Prompt:\n",
    "    context: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    names: list[str]\n",
    "    countries: list[str]\n",
    "\n",
    "\n",
    "def generate_prompt_pair(\n",
    "    num_entities,\n",
    "    all_names=names,\n",
    "    all_countries=list(countries_to_capitals.keys()),\n",
    "    parot=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        parot: bool\n",
    "            if true, will simplify the problem such that the answer\n",
    "            merely requires regurgitating the already specified city.\n",
    "            Otherwise, model needs to associate the countries with\n",
    "            their capitals.\n",
    "    \"\"\"\n",
    "    cs = random.sample(all_countries, num_entities*2)\n",
    "    ns = random.sample(all_names, num_entities*2)\n",
    "\n",
    "    cs_1 = cs[:num_entities]\n",
    "    ns_1 = ns[:num_entities]\n",
    "    cs_2 = cs[num_entities:]\n",
    "    ns_2 = ns[num_entities:]\n",
    "\n",
    "    if parot:\n",
    "        target_answers = [c for c in cs_1]\n",
    "        source_answers = [c for c in cs_2]\n",
    "        es_1 = [f\"{n} lives in {c}.\" for c, n in zip(target_answers, ns_1)]\n",
    "        es_2 = [f\"{n} lives in {c}.\" for c, n in zip(source_answers, ns_2)]\n",
    "        target_questions = [f\"Question: What country does {n} live in?\" for n in ns_1]\n",
    "        source_questions = [f\"Question: What country does {n} live in?\" for n in ns_2]\n",
    "    else:\n",
    "        es_1 = [f\"{n} lives in the capital city of {c}.\" for c, n in zip(cs_1, ns_1)]\n",
    "        es_2 = [f\"{n} lives in the capital city of {c}.\" for c, n in zip(cs_2, ns_2)]\n",
    "        target_answers = [countries_to_capitals[c] for c in cs_1]\n",
    "        source_answers = [countries_to_capitals[c] for c in cs_2]\n",
    "        target_questions = [f\"Question: Which city does {n} live in?\" for n in ns_1]\n",
    "        source_questions = [f\"Question: Which city does {n} live in?\" for n in ns_2]\n",
    "    target_context =  f\"Answer the question based on the context below. Keep the answer short.\\n\\nContext: {' '.join(es_1)}\"\n",
    "    source_context =  f\"Answer the question based on the context below. Keep the answer short.\\n\\nContext: {' '.join(es_2)}\"\n",
    "    return (Prompt(\n",
    "        context=target_context,\n",
    "        questions=target_questions,\n",
    "        answers=target_answers,\n",
    "        names=ns_1,\n",
    "        countries=cs_1,\n",
    "    ), Prompt(\n",
    "        context=source_context,\n",
    "        questions=source_questions,\n",
    "        answers=source_answers,\n",
    "        names=ns_2,\n",
    "        countries=cs_2,\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa094fc-100e-4272-b195-f2a19540a903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parot = False # if true, task will be to predicted the provided attributes. if false, predict cities from attributes\n",
    "target_prompt, source_prompt = generate_prompt_pair(2, parot=parot)\n",
    "print(\"Target:\", target_prompt.context)\n",
    "print(\"Questions:\", target_prompt.questions)\n",
    "print(\"Answers:\", target_prompt.answers)\n",
    "print()\n",
    "print(\"Source:\", source_prompt.context)\n",
    "print(\"Questions:\", source_prompt.questions)\n",
    "print(\"Answers\", source_prompt.answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93330d",
   "metadata": {},
   "source": [
    "# Patching experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb20ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_outputs(layer: int, save_map: dict):\n",
    "    \"\"\"Hook function for saving the output of a model component\"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            proj_output = output[0]\n",
    "        else:\n",
    "            proj_output = output\n",
    "        save_map[layer] = proj_output.clone()\n",
    "        return output\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def save_hidden_states(ids, model):\n",
    "    \"\"\"Save the hidden states of a model. Returns model output and a (num_layers,num_tokens,hidden_size) tensor of hidden states.\"\"\"\n",
    "    hooks = []\n",
    "    hidden_states = {}\n",
    "    model_layers = model.gpt_neox.layers if \"pythia\" in model_name else model.model.layers\n",
    "    for layer,model_layer in enumerate(model_layers):\n",
    "        hidden_state_hook_handle = model_layer.register_forward_hook(\n",
    "            save_outputs(layer, hidden_states)\n",
    "        )\n",
    "        hooks.append(hidden_state_hook_handle)\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            ids,\n",
    "            attention_mask=torch.ones_like(ids),\n",
    "        )\n",
    "    clear_hooks(hooks)\n",
    "    hidden_states_tensor = torch.stack([hidden_states[i] for i in range(len(hidden_states))], dim=0).squeeze(1)\n",
    "    return out, hidden_states_tensor\n",
    "\n",
    "\n",
    "def replace_outputs(layer: int, target_layers: list[int], target_position: int, original_hidden_states: dict, modified_hidden_states: dict):\n",
    "    \"\"\"Hook function for replacing hidden states of a target layer at a target position with the hidden states from a modified prompt.\"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            proj_output = output[0]\n",
    "        else:\n",
    "            proj_output = output\n",
    "\n",
    "        unsqueeze = len(proj_output.shape)>2\n",
    "        proj_output = proj_output.squeeze()\n",
    "        #proj_output = original_hidden_states[layer].clone()\n",
    "        if layer in target_layers:\n",
    "            proj_output[target_position:target_position+2,:] = modified_hidden_states[layer,target_position:target_position+2,:].clone()\n",
    "        if unsqueeze: proj_output = proj_output[None]\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            return (proj_output,) + output[1:]\n",
    "        else:\n",
    "            return proj_output\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def replace_hidden_states(ids, model, target_position, original_hidden_states, modified_hidden_states):\n",
    "    \"\"\"Replace the hidden states of a model. Returns model output.\"\"\"\n",
    "    hooks = []\n",
    "    model_layers = model.gpt_neox.layers if \"pythia\" in model_name else model.model.layers\n",
    "    for layer,model_layer in enumerate(model_layers):\n",
    "        hidden_state_hook_handle = model_layer.register_forward_hook(\n",
    "            replace_outputs(\n",
    "                layer,\n",
    "                list(range(len(model_layers))),\n",
    "                target_position,\n",
    "                original_hidden_states,\n",
    "                modified_hidden_states\n",
    "            )\n",
    "        )\n",
    "        hooks.append(hidden_state_hook_handle)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(\n",
    "                ids,\n",
    "                attention_mask=torch.ones_like(ids),\n",
    "            )\n",
    "        except:\n",
    "            clear_hooks(hooks)\n",
    "            assert False\n",
    "    clear_hooks(hooks)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59931da4-10fa-4390-ac67-93b5d0bb4f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def manual_decode(token_ids, tokenizer):\n",
    "    decoded = []\n",
    "    for tok in token_ids:\n",
    "        decoded.append(tokenizer.decode(tok))\n",
    "    return decoded\n",
    "        \n",
    "def batch_manual_decode(token_ids, tokenizer):\n",
    "    all_decoded = []\n",
    "    for samp in range(len(token_ids)):\n",
    "        decoded = manual_decode(token_ids[samp], tokenizer)\n",
    "        all_decoded.append(decoded)\n",
    "    return all_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d63df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "rows = []\n",
    "nruns = 100 if not debug else 5\n",
    "for run in tqdm.tqdm(range(nruns)):\n",
    "    \n",
    "    mp, ap = generate_prompt_pair(2)\n",
    "    for query_entity_idx in range(4):\n",
    "        \n",
    "        query_entities = mp.names + ap.names\n",
    "        questions = mp.questions + ap.questions\n",
    "        if parot:\n",
    "            main_prompt = mp.context + \"\\n\" + questions[query_entity_idx] + f\"\\nAnswer: {query_entities[query_entity_idx]} lives in\"\n",
    "            alt_prompt = ap.context + \"\\n\" + questions[query_entity_idx] + f\"\\nAnswer: {query_entities[query_entity_idx]} lives in\"\n",
    "        else:\n",
    "            main_prompt = mp.context + \"\\n\" + questions[query_entity_idx] + f\"\\nAnswer: {query_entities[query_entity_idx]} lives in the city of\"\n",
    "            alt_prompt = ap.context + \"\\n\" + questions[query_entity_idx] + f\"\\nAnswer: {query_entities[query_entity_idx]} lives in the city of\"\n",
    "\n",
    "        main_prompt_tokens = tokenizer(main_prompt, return_tensors=\"pt\").to(device)\n",
    "        alt_prompt_tokens = tokenizer(alt_prompt, return_tensors=\"pt\").to(device)\n",
    "        main_ids = main_prompt_tokens[\"input_ids\"]\n",
    "        alt_ids = alt_prompt_tokens[\"input_ids\"]\n",
    "\n",
    "        #main_decoded_tokens = tokenizer.convert_ids_to_tokens(main_prompt_tokens[\"input_ids\"][0])\n",
    "        main_decoded_tokens = manual_decode(main_prompt_tokens[\"input_ids\"][0], tokenizer)\n",
    "        main_n1_index = main_decoded_tokens.index(f\"{space_token}{mp.names[0]}\")\n",
    "        main_n2_index = main_decoded_tokens.index(f\"{space_token}{mp.names[1]}\")\n",
    "        main_c1_index = main_decoded_tokens.index(f\"{space_token}{mp.countries[0]}\")\n",
    "        main_c2_index = main_decoded_tokens.index(f\"{space_token}{mp.countries[1]}\")\n",
    "        \n",
    "        #alt_decoded_tokens = tokenizer.convert_ids_to_tokens(alt_prompt_tokens[\"input_ids\"][0])\n",
    "        alt_decoded_tokens = manual_decode(alt_prompt_tokens[\"input_ids\"][0], tokenizer)\n",
    "        alt_n1_index = alt_decoded_tokens.index(f\"{space_token}{ap.names[0]}\")\n",
    "        alt_n2_index = alt_decoded_tokens.index(f\"{space_token}{ap.names[1]}\")\n",
    "        alt_c1_index = alt_decoded_tokens.index(f\"{space_token}{ap.countries[0]}\")\n",
    "        alt_c2_index = alt_decoded_tokens.index(f\"{space_token}{ap.countries[1]}\")\n",
    "        if not (main_n1_index == alt_n1_index and main_n2_index == alt_n2_index and main_c1_index == alt_c1_index and main_c2_index == alt_c2_index):\n",
    "            print(\"Mismatch in indices\")\n",
    "            continue\n",
    "\n",
    "        main_a1_id = tokenizer(\" \" + mp.answers[0], add_special_tokens=False)[\"input_ids\"][0]\n",
    "        main_a2_id = tokenizer(\" \" + mp.answers[1], add_special_tokens=False)[\"input_ids\"][0]\n",
    "        alt_a1_id = tokenizer(\" \" + ap.answers[0], add_special_tokens=False)[\"input_ids\"][0]\n",
    "        alt_a2_id = tokenizer(\" \" + ap.answers[1], add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "        # Get hidden states\n",
    "        with torch.no_grad():\n",
    "            main_out, main_hidden_states = save_hidden_states(main_ids, model)\n",
    "            alt_out, alt_hidden_states = save_hidden_states(alt_ids, model)\n",
    "\n",
    "        for modify_index in [main_n1_index, main_n2_index, main_c1_index, main_c2_index]:\n",
    "\n",
    "            # Modify hidden states by replacing with alt_hidden_states\n",
    "            modified_out = replace_hidden_states(main_ids, model, modify_index, main_hidden_states, alt_hidden_states)\n",
    "            modified_log_probs = F.log_softmax(modified_out.logits[0,-1,:], dim=-1)\n",
    "            rows += [\n",
    "                {\n",
    "                    \"run\": run,\n",
    "                    \"query_name\": [\"e0\", \"e1\", \"e0'\", \"e1'\"][query_entity_idx],\n",
    "                    \"attribute\": [\"a0\", \"a1\", \"a0'\", \"a1'\"][i],\n",
    "                    \"swap_type\": \"entity\" if modify_index in [main_n1_index, main_n2_index] else \"attribute\",\n",
    "                    \"swap_index\": 0 if modify_index in [main_n1_index, main_c1_index] else 1,\n",
    "                    \"log_prob\": modified_log_probs[a_id].item(),\n",
    "                } for i, a_id in enumerate([main_a1_id, main_a2_id, alt_a1_id, alt_a2_id])\n",
    "            ]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not os.path.exists(f\"results/table-1/{model_alias}\"):\n",
    "        os.makedirs(f\"results/table-1/{model_alias}\")\n",
    "    df.to_csv(f\"results/table-1/{model_alias}/factorizability.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd7d14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"results/table-1/{model_alias}/factorizability.csv\")\n",
    "accuracy_df = df.groupby([\"query_name\", \"attribute\", \"swap_type\", \"swap_index\"]).mean().reset_index()\n",
    "\n",
    "# Create subplots for each combination of swap_type and swap_index\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Log Probability Heatmaps by Swap Type and Index', fontsize=16)\n",
    "\n",
    "# Define the order for consistent plotting\n",
    "attribute_order = ['a0', 'a1', \"a0'\", \"a1'\"]\n",
    "query_order = ['e0', 'e1', \"e0'\", \"e1'\"]\n",
    "\n",
    "# Get unique combinations of swap_type and swap_index\n",
    "swap_combinations = accuracy_df[['swap_type', 'swap_index']].drop_duplicates().sort_values(['swap_type', 'swap_index'])\n",
    "\n",
    "for idx, (_, row) in enumerate(swap_combinations.iterrows()):\n",
    "    swap_type = row['swap_type']\n",
    "    swap_index = row['swap_index']\n",
    "    \n",
    "    # Filter data for this combination\n",
    "    subset = accuracy_df[(accuracy_df['swap_type'] == swap_type) & (accuracy_df['swap_index'] == swap_index)]\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_data = subset.pivot(index='query_name', columns='attribute', values='log_prob')\n",
    "    \n",
    "    # Reorder according to specified order\n",
    "    pivot_data = pivot_data.reindex(index=query_order, columns=attribute_order)\n",
    "    \n",
    "    # Determine subplot position\n",
    "    row_idx = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_data, \n",
    "                annot=True, \n",
    "                fmt='.3f', \n",
    "                cmap='viridis',\n",
    "                ax=axes[row_idx, col_idx],\n",
    "                cbar_kws={'label': 'Log Probability'})\n",
    "    \n",
    "    axes[row_idx, col_idx].set_title(f'Swap Type: {swap_type}, Index: {swap_index}')\n",
    "    axes[row_idx, col_idx].set_xlabel('Attribute')\n",
    "    axes[row_idx, col_idx].set_ylabel('Query Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06388e2",
   "metadata": {},
   "source": [
    "#### Additivity replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c1a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_outputs(\n",
    "    layer: int,\n",
    "    target_layers: list[int],\n",
    "    pos1: int,\n",
    "    pos2: int,\n",
    "    original_hidden_states: torch.tensor,\n",
    "    delta: torch.tensor,\n",
    "    save_hidden_states: dict,\n",
    "    alt_pos1: int=None,\n",
    "    alt_pos2: int=None,\n",
    "    alt_delta: torch.tensor=None,\n",
    "):\n",
    "    \"\"\"Hook function for adding/subtracting a delta to the hidden states of a target layer at a target position.\"\"\"\n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            proj_output = output[0]\n",
    "        else:\n",
    "            proj_output = output\n",
    "\n",
    "        if layer in target_layers:\n",
    "            og_output = original_hidden_states[layer].clone()\n",
    "            unsqueeze = len(proj_output.shape)>2\n",
    "            proj_output = proj_output.squeeze()\n",
    "            \n",
    "            # Intervene\n",
    "            proj_output[pos1:pos1+2,:] = og_output[pos1:pos1+2,:] + delta[layer,:].clone()\n",
    "            proj_output[pos2:pos2+2,:] = og_output[pos2:pos2+2,:] - delta[layer,:].clone()\n",
    "            \n",
    "            # Intervene again\n",
    "            if alt_pos1 is not None:\n",
    "                proj_output[alt_pos1:alt_pos1+2,:] = og_output[alt_pos1:alt_pos1+2,:] + alt_delta[layer,:].clone()\n",
    "                proj_output[alt_pos2:alt_pos2+2,:] = og_output[alt_pos2:alt_pos2+2,:] - alt_delta[layer,:].clone()\n",
    "                \n",
    "            save_hidden_states[layer] = proj_output.clone()\n",
    "            if unsqueeze: proj_output = proj_output[None]\n",
    "        \n",
    "        if isinstance(proj_output, tuple):\n",
    "            return (proj_output,) + output[1:]\n",
    "        else:\n",
    "            return proj_output\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "def modify_hidden_states(\n",
    "    ids,\n",
    "    model,\n",
    "    e0_pos,\n",
    "    e1_pos,\n",
    "    a0_pos,\n",
    "    a1_pos,\n",
    "    original_hidden_states,\n",
    "    entity_delta,\n",
    "    attribute_delta,\n",
    "    condition,\n",
    "):\n",
    "    \"\"\"Modify the hidden states of a model by adding a delta to the original hidden states. Returns model output.\"\"\"\n",
    "    hooks = []\n",
    "    hidden_states = {}\n",
    "    model_layers = model.gpt_neox.layers if \"pythia\" in model_name else model.model.layers\n",
    "    for layer,model_layer in enumerate(model_layers):\n",
    "        if \"both\" in condition:\n",
    "            hidden_state_hook_handle = model_layer.register_forward_hook(\n",
    "                modify_outputs(\n",
    "                    layer=layer,\n",
    "                    target_layers=list(range(len(model_layers))),\n",
    "                    pos1=a0_pos,\n",
    "                    pos2=a1_pos,\n",
    "                    alt_pos1=e0_pos,\n",
    "                    alt_pos2=e1_pos,\n",
    "                    original_hidden_states=original_hidden_states,\n",
    "                    delta=attribute_delta,\n",
    "                    alt_delta=entity_delta,\n",
    "                    save_hidden_states=hidden_states,\n",
    "                )\n",
    "            )\n",
    "            hooks.append(hidden_state_hook_handle)\n",
    "        elif \"attribute\" in condition:\n",
    "            hidden_state_hook_handle = model_layer.register_forward_hook(\n",
    "                modify_outputs(\n",
    "                    layer=layer,\n",
    "                    target_layers=list(range(len(model_layers))),\n",
    "                    pos1=a0_pos,\n",
    "                    pos2=a1_pos,\n",
    "                    original_hidden_states=original_hidden_states,\n",
    "                    delta=attribute_delta,\n",
    "                    save_hidden_states=hidden_states,\n",
    "                )\n",
    "            )\n",
    "            hooks.append(hidden_state_hook_handle)\n",
    "        elif \"entity\" in condition:\n",
    "            hidden_state_hook_handle = model_layer.register_forward_hook(\n",
    "                modify_outputs(\n",
    "                    layer=layer,\n",
    "                    target_layers=list(range(len(model_layers))),\n",
    "                    pos1=e0_pos,\n",
    "                    pos2=e1_pos,\n",
    "                    original_hidden_states=original_hidden_states,\n",
    "                    delta=entity_delta,\n",
    "                    save_hidden_states=hidden_states,\n",
    "                )\n",
    "            )\n",
    "            hooks.append(hidden_state_hook_handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid condition: {condition}\")\n",
    "    with torch.no_grad():\n",
    "        out = model(\n",
    "            ids,\n",
    "            attention_mask=torch.ones_like(ids),\n",
    "        )\n",
    "    hidden_states_tensor = torch.stack([hidden_states[i] for i in range(len(hidden_states))], dim=0).squeeze(1)\n",
    "    clear_hooks(hooks)\n",
    "    return out, hidden_states_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "# Allocate 'train' and 'test' names and countries for calculating means\n",
    "train_names = names[:len(names)//2]\n",
    "test_names = names[len(names)//2:]\n",
    "train_countries = list(countries_to_capitals.keys())[:len(countries_to_capitals)//2]\n",
    "test_countries = list(countries_to_capitals.keys())[len(countries_to_capitals)//2:]\n",
    "\n",
    "# Calculate mean differences\n",
    "num_runs = 500\n",
    "attribute_deltas = []\n",
    "entity_deltas = []\n",
    "\n",
    "it = tqdm.tqdm(range(5)) if debug else tqdm.tqdm(range(num_runs))\n",
    "for _ in it:\n",
    "\n",
    "    mp, ap = generate_prompt_pair(2, train_names, train_countries, parot=parot)\n",
    "\n",
    "    for query_entity_idx in range(2):\n",
    "\n",
    "        if parot:\n",
    "            main_prompt = mp.context + \"\\n\" + mp.questions[query_entity_idx] + f\"\\nAnswer: {mp.names[query_entity_idx]} lives in\"\n",
    "            alt_prompt = ap.context + \"\\n\" + ap.questions[query_entity_idx] + f\"\\nAnswer: {ap.names[query_entity_idx]} lives in\"\n",
    "        else:\n",
    "            main_prompt = mp.context + \"\\n\" + mp.questions[query_entity_idx] + f\"\\nAnswer: {mp.names[query_entity_idx]} lives in the city of\"\n",
    "            alt_prompt = ap.context + \"\\n\" + ap.questions[query_entity_idx] + f\"\\nAnswer: {ap.names[query_entity_idx]} lives in the city of\"\n",
    "        \n",
    "        main_prompt_tokens = tokenizer(main_prompt, return_tensors=\"pt\").to(device)\n",
    "        alt_prompt_tokens = tokenizer(alt_prompt, return_tensors=\"pt\").to(device)\n",
    "        main_ids = main_prompt_tokens[\"input_ids\"]\n",
    "        alt_ids = alt_prompt_tokens[\"input_ids\"]\n",
    "\n",
    "        main_decoded_tokens = manual_decode(main_prompt_tokens[\"input_ids\"][0], tokenizer)\n",
    "        alt_decoded_tokens =  manual_decode(alt_prompt_tokens[\"input_ids\"][0], tokenizer)\n",
    "        main_n1_index = main_decoded_tokens.index(f\"{space_token}{mp.names[0]}\")\n",
    "        main_n2_index = main_decoded_tokens.index(f\"{space_token}{mp.names[1]}\")\n",
    "        main_c1_index = main_decoded_tokens.index(f\"{space_token}{mp.countries[0]}\")\n",
    "        main_c2_index = main_decoded_tokens.index(f\"{space_token}{mp.countries[1]}\")\n",
    "        alt_n1_index = alt_decoded_tokens.index(f\"{space_token}{ap.names[0]}\")\n",
    "        alt_n2_index = alt_decoded_tokens.index(f\"{space_token}{ap.names[1]}\")\n",
    "        alt_c1_index = alt_decoded_tokens.index(f\"{space_token}{ap.countries[0]}\")\n",
    "        alt_c2_index = alt_decoded_tokens.index(f\"{space_token}{ap.countries[1]}\")\n",
    "        if not (main_n1_index == alt_n1_index and main_n2_index == alt_n2_index and main_c1_index == alt_c1_index and main_c2_index == alt_c2_index):\n",
    "            print(\"Mismatch in indices\")\n",
    "            continue\n",
    "\n",
    "        # Get hidden states\n",
    "        with torch.no_grad():\n",
    "            main_out, main_hidden_states = save_hidden_states(main_ids, model)\n",
    "            alt_out, alt_hidden_states = save_hidden_states(alt_ids, model)\n",
    "\n",
    "        attribute_deltas.append(main_hidden_states[:, main_c2_index:main_c2_index+2, :] - alt_hidden_states[:, alt_c1_index:alt_c1_index+2, :])\n",
    "        entity_deltas.append(   main_hidden_states[:, main_n2_index:main_n2_index+2, :] - alt_hidden_states[:, alt_n1_index:alt_n1_index+2, :])\n",
    "\n",
    "attribute_deltas = torch.stack(attribute_deltas, dim=0).mean(dim=0)\n",
    "entity_deltas = torch.stack(entity_deltas, dim=0).mean(dim=0)\n",
    "random_entity_deltas = torch.randn_like(entity_deltas) * entity_deltas.std()\n",
    "random_attribute_deltas = torch.randn_like(attribute_deltas) * attribute_deltas.std()\n",
    "\n",
    "\n",
    "# Calculate effect of mean interventions\n",
    "num_runs = 5 if debug else 100\n",
    "rows = []\n",
    "all_original_attribute_hidden_states = []\n",
    "all_original_entity_hidden_states = []\n",
    "all_intervened_attribute_hidden_states = []\n",
    "all_intervened_entity_hidden_states = []\n",
    "all_modified_attribute_hidden_states = []\n",
    "all_modified_entity_hidden_states = []\n",
    "for run in tqdm.tqdm(range(num_runs)):\n",
    "\n",
    "    p, _ = generate_prompt_pair(2, test_names, test_countries, parot=parot)\n",
    "\n",
    "    for query_entity_idx in range(2):\n",
    "\n",
    "        if parot:\n",
    "            prompt = p.context + \"\\n\" + p.questions[query_entity_idx] + f\"\\nAnswer: {p.names[query_entity_idx]} lives in\"\n",
    "        else:\n",
    "            prompt = p.context + \"\\n\" + p.questions[query_entity_idx] + f\"\\nAnswer: {p.names[query_entity_idx]} lives in the city of\"\n",
    "\n",
    "        # Get tokens\n",
    "        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        ids = prompt_tokens[\"input_ids\"]\n",
    "        a1_id = tokenizer(\" \" + p.answers[0], add_special_tokens=False)[\"input_ids\"][0]\n",
    "        a2_id = tokenizer(\" \" + p.answers[1], add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "        # Get token positions of entities and attributes\n",
    "        decoded_tokens = manual_decode(ids[0], tokenizer)\n",
    "        e0_pos = decoded_tokens.index(f\"{space_token}{p.names[0]}\")\n",
    "        e1_pos = decoded_tokens.index(f\"{space_token}{p.names[1]}\")\n",
    "        a0_pos = decoded_tokens.index(f\"{space_token}{p.countries[0]}\")\n",
    "        a1_pos = decoded_tokens.index(f\"{space_token}{p.countries[1]}\")\n",
    "\n",
    "        # control condition\n",
    "        with torch.no_grad():\n",
    "            out, original_hidden_states = save_hidden_states(ids, model)\n",
    "            logprobs = F.log_softmax(out.logits[0, -1], dim=-1)\n",
    "        rows.append({\n",
    "            \"run_id\": run,\n",
    "            \"condition\": \"control\",\n",
    "            \"query_entity_idx\": query_entity_idx,\n",
    "            \"max_logprob_token\": tokenizer.decode(logprobs.argmax().item()),\n",
    "            \"max_logprob\": logprobs.max().item(),\n",
    "            \"a1_token\": tokenizer.decode(a1_id),\n",
    "            \"a1_logprob\": logprobs[a1_id].item(),\n",
    "            \"a2_token\": tokenizer.decode(a2_id),\n",
    "            \"a2_logprob\": logprobs[a2_id].item(),\n",
    "            \"correct_answer\": \"a1\" if query_entity_idx == 0 else \"a2\",\n",
    "        })\n",
    "        all_original_attribute_hidden_states.append(original_hidden_states[:, [a0_pos, a0_pos+1, a1_pos, a1_pos+1], :].cpu())\n",
    "        all_original_entity_hidden_states.append(   original_hidden_states[:, [e0_pos, e0_pos+1, e1_pos, e1_pos+1], :].cpu())\n",
    "\n",
    "        # Modified conditions\n",
    "        for condition in [\"attribute\", \"entity\", \"both\", \"random-attribute\", \"random-entity\", \"random-both\"]:\n",
    "            if \"random\" in condition:\n",
    "                out, intervened_hidden_states = modify_hidden_states(\n",
    "                    ids=ids,\n",
    "                    model=model,\n",
    "                    e0_pos=e0_pos,\n",
    "                    e1_pos=e1_pos,\n",
    "                    a0_pos=a0_pos,\n",
    "                    a1_pos=a1_pos,\n",
    "                    original_hidden_states=original_hidden_states,\n",
    "                    entity_delta=random_entity_deltas,\n",
    "                    attribute_delta=random_attribute_deltas,\n",
    "                    condition=condition,\n",
    "                )\n",
    "            else:\n",
    "                out, intervened_hidden_states = modify_hidden_states(\n",
    "                    ids=ids,\n",
    "                    model=model,\n",
    "                    e0_pos=e0_pos,\n",
    "                    e1_pos=e1_pos,\n",
    "                    a0_pos=a0_pos,\n",
    "                    a1_pos=a1_pos,\n",
    "                    original_hidden_states=original_hidden_states,\n",
    "                    entity_delta=entity_deltas,\n",
    "                    attribute_delta=attribute_deltas,\n",
    "                    condition=condition,\n",
    "                )\n",
    "            logprobs = F.log_softmax(out.logits[0, -1], dim=-1)\n",
    "            rows.append({\n",
    "                \"run_id\": run,\n",
    "                \"condition\": condition,\n",
    "                \"query_entity_idx\": query_entity_idx,\n",
    "                \"max_logprob_token\": tokenizer.decode(logprobs.argmax().item()),\n",
    "                \"max_logprob\": logprobs.max().item(),\n",
    "                \"a1_token\": tokenizer.decode(a1_id),\n",
    "                \"a1_logprob\": logprobs[a1_id].item(),\n",
    "                \"a2_token\": tokenizer.decode(a2_id),\n",
    "                \"a2_logprob\": logprobs[a2_id].item(),\n",
    "                \"correct_answer\": \"a1\" if query_entity_idx == 0 else \"a2\",\n",
    "            })\n",
    "\n",
    "            if \"random\" not in condition and \"both\" not in condition:\n",
    "                if \"attribute\" in condition:\n",
    "                    all_intervened_attribute_hidden_states.append(intervened_hidden_states[:, [a0_pos, a0_pos+1, a1_pos, a1_pos+1], :].cpu())\n",
    "                else:\n",
    "                    all_intervened_entity_hidden_states.append(intervened_hidden_states[:, [e0_pos, e0_pos+1, e1_pos, e1_pos+1], :].cpu())\n",
    "\n",
    "                if \"attribute\" in condition:\n",
    "                    attributes = p.countries[::-1]\n",
    "                    entities = p.names\n",
    "                else:\n",
    "                    attributes = p.countries\n",
    "                    entities = p.names[::-1]\n",
    "                if parot:\n",
    "                    modified_prompt = f\"Answer the question based on the context below. Keep the answer short.\\n\\nContext: {entities[0]} lives in {attributes[0]}. {entities[1]} lives in {attributes[1]}.\\nQuestion: Where does {p.names[query_entity_idx]} live?\\nAnswer: {p.names[query_entity_idx]} lives in\"\n",
    "                else:\n",
    "                    modified_prompt = f\"Answer the question based on the context below. Keep the answer short.\\n\\nContext: {entities[0]} lives in the capital city of {attributes[0]}. {entities[1]} lives in the capital city of {attributes[1]}.\\nQuestion: Which city does {p.names[query_entity_idx]} live in?\\nAnswer: {p.names[query_entity_idx]} lives in the city of\"\n",
    "                modified_prompt_tokens = tokenizer(modified_prompt, return_tensors=\"pt\").to(device)\n",
    "                modified_ids = modified_prompt_tokens[\"input_ids\"]\n",
    "                with torch.no_grad():\n",
    "                    _, modified_hidden_states = save_hidden_states(modified_ids, model)\n",
    "                if \"attribute\" in condition:\n",
    "                    all_modified_attribute_hidden_states.append(modified_hidden_states[:, [a0_pos, a0_pos+1, a1_pos, a1_pos+1], :].cpu())\n",
    "                else:\n",
    "                    all_modified_entity_hidden_states.append(modified_hidden_states[:, [e0_pos, e0_pos+1, e1_pos, e1_pos+1], :].cpu())\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(f\"results/table-1/{model_alias}/mean_interventions.csv\", index=False)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fadecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"results/table-1/{model_alias}/mean_interventions.csv\")\n",
    "\n",
    "def row_is_correct(row):\n",
    "    return (row[\"correct_answer\"] == \"a1\" and row[\"a1_logprob\"] > row[\"a2_logprob\"]) or (row[\"correct_answer\"] == \"a2\" and row[\"a2_logprob\"] > row[\"a1_logprob\"])\n",
    "\n",
    "for condition, cdf in df.groupby(\"condition\"):\n",
    "    accuracy = len([1 for _, row in cdf.iterrows() if row_is_correct(row)]) / len(cdf)\n",
    "    print(f\"{condition}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b52f01-28a6-4b73-b4ae-0c4890136f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_original_attribute_hidden_states_matrix =   torch.stack(all_original_attribute_hidden_states)\n",
    "all_original_entity_hidden_states_matrix =      torch.stack(all_original_entity_hidden_states)\n",
    "all_intervened_attribute_hidden_states_matrix = torch.stack(all_intervened_attribute_hidden_states)\n",
    "all_intervened_entity_hidden_states_matrix =    torch.stack(all_intervened_entity_hidden_states)\n",
    "all_modified_attribute_hidden_states_matrix =   torch.stack(all_modified_attribute_hidden_states)\n",
    "all_modified_entity_hidden_states_matrix =      torch.stack(all_modified_entity_hidden_states)\n",
    "\n",
    "print(all_original_attribute_hidden_states_matrix.shape)\n",
    "print(all_original_entity_hidden_states_matrix.shape)\n",
    "print(all_intervened_attribute_hidden_states_matrix.shape)\n",
    "print(all_intervened_entity_hidden_states_matrix.shape)\n",
    "print(all_modified_attribute_hidden_states_matrix.shape)\n",
    "print(all_modified_entity_hidden_states_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5f7ec-39e8-4359-8ed6-764200a9d10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_by_layer_and_position(natty_hstates, intrv_hstates, layer=None, pos=None):\n",
    "    d = natty_hstates.shape[-1]\n",
    "    if layer is not None:\n",
    "        natty_hstates = natty_hstates[:,layer]\n",
    "        intrv_hstates = intrv_hstates[:,layer]\n",
    "    if pos is None:\n",
    "        natty_states = [natty_hstates[i].reshape(-1,d) for i in range(len(natty_hstates))]\n",
    "        intrv_states = [intrv_hstates[i].reshape(-1,d) for i in range(len(natty_hstates))]\n",
    "    else:\n",
    "        natty_states = [natty_hstates[i][pos].reshape(-1,d) for i in range(len(natty_hstates))]\n",
    "        intrv_states = [intrv_hstates[i][pos].reshape(-1,d) for i in range(len(natty_hstates))]\n",
    "    natty_states = torch.vstack(natty_states)\n",
    "    intrv_states = torch.vstack(intrv_states)\n",
    "    return natty_states, intrv_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b646772-e5ac-4fc6-87c3-4d958310693d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_intervened_attribute_hidden_states_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7477c-9e62-4980-bca4-7fd881fc195c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from divergence_utils import visualize_states\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "n_samples = 10\n",
    "layer = None\n",
    "pos = None\n",
    "\n",
    "natty_hstates = torch.concat([\n",
    "    all_modified_attribute_hidden_states_matrix[:200],\n",
    "    all_modified_entity_hidden_states_matrix[:200],\n",
    "], dim=0)[::2]\n",
    "intrv_hstates = torch.concat([\n",
    "    all_intervened_attribute_hidden_states_matrix[:200],\n",
    "    all_intervened_entity_hidden_states_matrix[:200],\n",
    "], dim=0)[::2]\n",
    "\n",
    "emd_df_dict = {\n",
    "    \"sample_id\": [],\n",
    "    \"pos\": [],\n",
    "    \"layer\": [],\n",
    "    \"mse\": [],\n",
    "    \"emd\": [],\n",
    "    \"base_emd\": [],\n",
    "}\n",
    "\n",
    "\n",
    "print()\n",
    "for layer in range(natty_hstates.shape[1]):\n",
    "    natty_states, intrv_states = filter_by_layer_and_position(\n",
    "        natty_hstates, intrv_hstates, layer=layer, pos=pos\n",
    "    )\n",
    "    print(\"Layer:\", layer, \"Pos:\", pos)\n",
    "    print(\"Natty:\", natty_states.shape)\n",
    "    print(\"Intrv:\", intrv_states.shape)\n",
    "    for samp_id in range(n_samples):\n",
    "        p = str(pos)\n",
    "        diffs = visualize_states(\n",
    "            natty_states,\n",
    "            intrv_states,\n",
    "            xdim=0,\n",
    "            ydim=1,\n",
    "            save_name=f\"figs/mean_diff_layer{layer}_pos{p}.png\",\n",
    "            expl_var_threshold=0,\n",
    "            emd_sample_type=\"permute\",\n",
    "            emd_sample_size=len(natty_hstates)//2,\n",
    "            normalize_emd=True,\n",
    "            visualize=samp_id==0,\n",
    "            #verbose=samp_id==0,\n",
    "        )\n",
    "        mse = diffs[\"mse\"]\n",
    "        emd = diffs[\"emd\"]\n",
    "        base_emd = diffs[\"base_emd\"]\n",
    "        emd_df_dict[\"sample_id\"].append(samp_id)\n",
    "        emd_df_dict[\"emd\"].append(emd)\n",
    "        emd_df_dict[\"base_emd\"].append(base_emd)\n",
    "        emd_df_dict[\"mse\"].append(mse)\n",
    "        emd_df_dict[\"pos\"].append(pos)\n",
    "        emd_df_dict[\"layer\"].append(layer)\n",
    "    \n",
    "emd_df = pd.DataFrame(emd_df_dict)\n",
    "emd_df[[\"base_emd\",\"emd\",]].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eade27-910e-459e-babe-b8223f688914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = dict(emd_df[[\"base_emd\",\"emd\",]].mean())\n",
    "base_emd = d[\"base_emd\"]\n",
    "emd = d[\"emd\"]\n",
    "perc = (emd-base_emd)/base_emd\n",
    "frac = emd/base_emd\n",
    "print(\"EMD %:\", perc)\n",
    "print(\"EMD Frac:\", frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c10e47-d663-487b-8ec4-50f9eae57558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emd_df[\"diff\"] = emd_df[\"emd\"]-emd_df[\"base_emd\"]\n",
    "emd_df.groupby([\"layer\"])[[\"emd\", \"base_emd\", \"diff\"]].mean().reset_index().sort_values(by=\"diff\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30613d-993b-49d5-af87-4c1b3aad0c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emd_df[[\"emd\",\"base_emd\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f9d59-6dc7-4498-8b90-ddccd412d8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emd_df[\"method\"] = \"mean_diff\"\n",
    "if not os.path.exists(\"csvs/\"):\n",
    "    os.mkdir(\"csvs/\")\n",
    "emd_df.to_csv(\"csvs/mean_diff_individuated_layers_all_pos.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c62e9-e118-4edd-8e8f-802ab6bbb03a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c12f3-a75b-4340-b06a-98ac305c3929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from divergence_utils import visualize_states\n",
    "#\n",
    "#max_diff = 0\n",
    "#pos = None\n",
    "#for layer in range(9,14): #range(all_modified_attribute_hidden_states_matrix.shape[1]):\n",
    "#    natty_hstates = torch.concat([\n",
    "#        all_modified_attribute_hidden_states_matrix[:200],\n",
    "#        all_modified_entity_hidden_states_matrix[:200],\n",
    "#    ], dim=0)[::2]\n",
    "#    intrv_hstates = torch.concat([\n",
    "#        all_intervened_attribute_hidden_states_matrix[:200],\n",
    "#        all_intervened_entity_hidden_states_matrix[:200],\n",
    "#    ], dim=0)[::2]\n",
    "#    \n",
    "#    natty_states, intrv_states = filter_by_layer_and_position(\n",
    "#        natty_hstates, intrv_hstates, layer=layer, pos=pos\n",
    "#    )\n",
    "#    print()\n",
    "#    print(\"Layer:\", layer)\n",
    "#    diffs = visualize_states(\n",
    "#        natty_states,\n",
    "#        intrv_states,\n",
    "#        xdim=0,\n",
    "#        ydim=1,\n",
    "#        save_name=None,\n",
    "#        expl_var_threshold=0,\n",
    "#        sample_size=None,\n",
    "#    )\n",
    "#    mse = diffs[\"mse\"]\n",
    "#    emd = diffs[\"emd\"]\n",
    "#    base_emd = diffs[\"base_emd\"]\n",
    "#    print(\"MSE:\", mse)\n",
    "#    print(\"EMD:\", emd)\n",
    "#    print(\"BaseEMD:\", base_emd)\n",
    "#    \n",
    "#    diff = emd\n",
    "#    if diff>max_diff:\n",
    "#        max_layer = layer\n",
    "#        max_diff = diff\n",
    "#print(\"Max Diff Layer:\", max_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb629ce-5a5f-45da-8517-f3affda56c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf5d62-db10-4dc1-983b-abc0994cac6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pos = None # None means all\n",
    "#layer = None # None means all\n",
    "#\n",
    "#natty_hstates = torch.concat([\n",
    "#    all_modified_attribute_hidden_states_matrix[:200],\n",
    "#    all_modified_entity_hidden_states_matrix[:200],\n",
    "#], dim=0)[::2]\n",
    "#intrv_hstates = torch.concat([\n",
    "#    all_intervened_attribute_hidden_states_matrix[:200],\n",
    "#    all_intervened_entity_hidden_states_matrix[:200],\n",
    "#], dim=0)[::2]\n",
    "#\n",
    "#natty_states, intrv_states = filter_by_layer_and_position(\n",
    "#    natty_hstates, intrv_hstates, layer=layer, pos=pos\n",
    "#)\n",
    "#\n",
    "#diffs = visualize_states(\n",
    "#    natty_states,\n",
    "#    intrv_states,\n",
    "#    xdim=0,\n",
    "#    ydim=1,\n",
    "#    save_name=\"all_states_all_positions.png\",\n",
    "#    expl_var_threshold=0,\n",
    "#    sample_size=None,\n",
    "#)\n",
    "#for\n",
    "#mse = diffs[\"mse\"]\n",
    "#emd = diffs[\"emd\"]\n",
    "#base_emd = diffs[\"base_emd\"]\n",
    "#print(\"MSE:\", mse)\n",
    "#print(\"EMD:\", emd)\n",
    "#print(\"BaseEMD:\", base_emd)\n",
    "#df = pd.DataFrame({\n",
    "#    \"condition\": [\"Mean Diff\"],\n",
    "#    \"emd\": [emd],\n",
    "#    \"base_emd\": [base_emd],\n",
    "#})\n",
    "#df.to_csv(\"mean_diff.csv\", header=True, index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95959bc-8455-4abc-919d-e57c56d9c9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
